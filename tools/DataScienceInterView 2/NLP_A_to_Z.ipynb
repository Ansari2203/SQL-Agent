{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfP0dNsNGAfO"
      },
      "source": [
        "# f string formating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRhaBqRq8ODw",
        "outputId": "61be016c-ae0c-4ab0-9e61-b9e5d34d9f9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My Name is Juned Ansari\n",
            "My Name is Juned Ansari and Age: 33\n",
            "\n",
            "\n",
            "Before Formating:\n",
            "xyz Geography 300\n",
            "pqr Math 200\n",
            "Author Topic Pages\n",
            "Abc History 1200\n",
            "\n",
            "\n",
            "After Formating:\n",
            "xyz        Geography              300\n",
            "pqr        Math                   200\n",
            "Author     Topic                Pages\n",
            "Abc        History               1200\n"
          ]
        }
      ],
      "source": [
        "# f string - New after python 3.6\n",
        "name = \"Juned Ansari\"\n",
        "print(f\"My Name is {name}\")\n",
        "\n",
        "mydict = {\n",
        "    'name':'Juned Ansari',\n",
        "    'age':33\n",
        "}\n",
        "\n",
        "print(f\"My Name is {mydict['name']} and Age: {mydict['age']}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "# formating using f string\n",
        "library = {\n",
        "    ('Author','Topic','Pages'),\n",
        "    ('Abc','History',1200),\n",
        "    ('xyz','Geography',300),\n",
        "    ('pqr','Math',200)\n",
        "}\n",
        "\n",
        "# Before Formating\n",
        "print(\"Before Formating:\")\n",
        "for author,topic,pages in library:\n",
        "    print(f\"{author} {topic} {pages}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"After Formating:\")\n",
        "# After Formating\n",
        "for author,topic,pages in library:\n",
        "    print(f\"{author:{10}} {topic:{20}} {pages:{5}}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RUH1TVZ79tI"
      },
      "source": [
        "# Working with Text File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "entkSjtz6WYN",
        "outputId": "4b82c2af-de0b-4763-c3f7-cce17c45e414"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tCsXuElFwwC",
        "outputId": "86a5bb5d-3969-4615-cba3-f026586466f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing sample.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile sample.txt\n",
        "This is sample text file\n",
        "my name is juned ansari\n",
        "i am a data scientist\n",
        "my qualification is mscit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3BfpGbMG56V",
        "outputId": "8a5b394d-099f-48db-917f-8f6a6d0bdafa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is sample text file \n",
            "\n",
            "my name is juned ansari \n",
            "\n",
            "i am a data scientist\n",
            "\n",
            "my qualification is mscit.\n"
          ]
        }
      ],
      "source": [
        "#Read File\n",
        "file = open(\"sample.txt\")\n",
        "for line in file.readlines():\n",
        "  print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svXd6Rw58hMj",
        "outputId": "701fc5bb-3495-4284-fcd7-98c37f31233b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is sample text file \n",
            "my name is juned ansari \n",
            "i am a data scientist\n",
            "my qualification is mscit.New Line Appended\n"
          ]
        }
      ],
      "source": [
        "# Append file\n",
        "file = open(\"sample.txt\",\"a+\")\n",
        "file.write(\"New Line Appended\")\n",
        "file.close()\n",
        "\n",
        "myfile = open(\"sample.txt\")\n",
        "print(myfile.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhQAj2HO9AH3",
        "outputId": "d6eaf9bc-abea-4e91-ccdc-1040f0485994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is sample text file \n",
            "\n",
            "my name is juned ansari \n",
            "\n",
            "i am a data scientist\n",
            "\n",
            "my qualification is mscit.New Line Appended\n"
          ]
        }
      ],
      "source": [
        "myfile.seek(0)\n",
        "content = myfile.readlines()\n",
        "for line in content:\n",
        "    print(line)\n",
        "myfile.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsMEBS1D-7e3"
      },
      "source": [
        "# Working Regular Expression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlHPnyi4KnVs",
        "outputId": "35f3a064-20b3-4390-b4c3-8d7c1f58f01c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "990-477-3679\n",
            "990\n",
            "477\n",
            "3679\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "text = \"Hello india My Mobile Number is 990-477-3679 and roll number is 03\"\n",
        "pattern = '(\\d{3})-(\\d{3})-(\\d{4})'\n",
        "print(re.search(pattern,text).group())\n",
        "print(re.search(pattern,text).group(1))\n",
        "print(re.search(pattern,text).group(2))\n",
        "print(re.search(pattern,text).group(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KHE132mKosM",
        "outputId": "e31167e9-dca2-4d23-84ff-c14b9d0b5f56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('990', '477', '3679')\n"
          ]
        }
      ],
      "source": [
        "for data in re.findall(pattern,text):\n",
        "  print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WjJDiYIKqL4",
        "outputId": "b2481cb1-2654-4672-8738-38b170d60ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Apple\n"
          ]
        }
      ],
      "source": [
        "print(re.search(\"Apple|Mango\",\"I like Apple and Mango\").group())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkPS6xERMGF5",
        "outputId": "5e128be3-e386-47dc-db61-739af4fee13b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Apple', 'Mango']\n"
          ]
        }
      ],
      "source": [
        "print(re.findall(\"Apple|Mango\",\"I like Apple and Mango\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbWaEmig7r6k"
      },
      "source": [
        "# Working with Spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWdaci1V-hJk"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "3tr-8JhOKQKR",
        "outputId": "f6dcc553-a1e2-43de-e4ba-9faa1a1a9bc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello\n",
            "World\n",
            ",\n",
            "A\n",
            "5\n",
            "km\n",
            "cab\n",
            "ride\n",
            "to\n",
            "cost\n",
            "$\n",
            "10\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"61492ad331d441ebae9727963cc03b19-0\" class=\"displacy\" width=\"1370\" height=\"357.0\" direction=\"ltr\" style=\"max-width: none; height: 357.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"160\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"160\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"270\">going</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"270\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"380\">to</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"380\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"490\">build</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"490\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"600\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"600\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"710\">U.K.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"710\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"820\">factory</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"820\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"930\">for</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"930\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1040\">$</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1040\">SYM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1150\">6</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1150\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1260\">million.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1260\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-61492ad331d441ebae9727963cc03b19-0-0\" stroke-width=\"2px\" d=\"M70,222.0 C70,112.0 260.0,112.0 260.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-61492ad331d441ebae9727963cc03b19-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,224.0 L62,212.0 78,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-61492ad331d441ebae9727963cc03b19-0-1\" stroke-width=\"2px\" d=\"M180,222.0 C180,167.0 255.0,167.0 255.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-61492ad331d441ebae9727963cc03b19-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M180,224.0 L172,212.0 188,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-61492ad331d441ebae9727963cc03b19-0-2\" stroke-width=\"2px\" d=\"M400,222.0 C400,167.0 475.0,167.0 475.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-61492ad331d441ebae9727963cc03b19-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M400,224.0 L392,212.0 408,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-61492ad331d441ebae9727963cc03b19-0-3\" stroke-width=\"2px\" d=\"M290,222.0 C290,112.0 480.0,112.0 480.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-61492ad331d441ebae9727963cc03b19-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M480.0,224.0 L488.0,212.0 472.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-61492ad331d441ebae9727963cc03b19-0-4\" stroke-width=\"2px\" d=\"M620,222.0 C620,112.0 810.0,112.0 810.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-61492ad331d441ebae9727963cc03b19-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M620,224.0 L612,212.0 628,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-61492ad331d441ebae9727963cc03b19-0-5\" stroke-width=\"2px\" d=\"M730,222.0 C730,167.0 805.0,167.0 805.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-61492ad331d441ebae9727963cc03b19-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M730,224.0 L722,212.0 738,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-61492ad331d441ebae9727963cc03b19-0-6\" stroke-width=\"2px\" d=\"M510,222.0 C510,57.0 815.0,57.0 815.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-61492ad331d441ebae9727963cc03b19-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M815.0,224.0 L823.0,212.0 807.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-61492ad331d441ebae9727963cc03b19-0-7\" stroke-width=\"2px\" d=\"M510,222.0 C510,2.0 930.0,2.0 930.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-61492ad331d441ebae9727963cc03b19-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M930.0,224.0 L938.0,212.0 922.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-61492ad331d441ebae9727963cc03b19-0-8\" stroke-width=\"2px\" d=\"M1060,222.0 C1060,112.0 1250.0,112.0 1250.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-61492ad331d441ebae9727963cc03b19-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1060,224.0 L1052,212.0 1068,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-61492ad331d441ebae9727963cc03b19-0-9\" stroke-width=\"2px\" d=\"M1170,222.0 C1170,167.0 1245.0,167.0 1245.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-61492ad331d441ebae9727963cc03b19-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1170,224.0 L1162,212.0 1178,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-61492ad331d441ebae9727963cc03b19-0-10\" stroke-width=\"2px\" d=\"M950,222.0 C950,57.0 1255.0,57.0 1255.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-61492ad331d441ebae9727963cc03b19-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1255.0,224.0 L1263.0,212.0 1247.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entity Display:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is going to build a \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    U.K.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " factory for \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $6 million\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              ".</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Import the English Language Class\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(\"Hello World, A 5km cab ride to cost $10\")\n",
        "for token in doc:\n",
        "  print(token.text)\n",
        "\n",
        "#To Display Dependency\n",
        "from spacy import displacy\n",
        "doc = nlp(u'Apple is going to build a U.K. factory for $6 million.')\n",
        "displacy.render(doc, style='dep', jupyter=True, options={'distance': 110})\n",
        "\n",
        "#To Display Entity\n",
        "print(\"Entity Display:\")\n",
        "displacy.render(doc, style='ent', jupyter=True, options={'distance': 110})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz2rKBRTKv3O",
        "outputId": "541a596d-34fe-4054-db5a-88e9ddbdd9dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence Token: [It cost $5 million and Companies Email is ansarijuned4all@gmail.com, and i live in india, , right now I am eating mango]\n",
            "Token: ['It', 'cost', '$', '5', 'million', 'and', 'Companies', 'Email', 'is', 'ansarijuned4all@gmail.com', 'and', 'i', 'live', 'in', 'india', ',', 'right', 'now', 'I', 'am', 'eating', 'mango']\n",
            "Token Index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
            "\n",
            "\n",
            "Named Entity Recongnisation(NER):\n",
            "$5 million :::Label(N E R)::: MONEY ::: Explain::: Monetary values, including unit\n",
            "Companies Email :::Label(N E R)::: ORG ::: Explain::: Companies, agencies, institutions, etc.\n",
            "ansarijuned4all@gmail.com :::Label(N E R)::: ORG ::: Explain::: Companies, agencies, institutions, etc.\n",
            "india :::Label(N E R)::: GPE ::: Explain::: Countries, cities, states\n",
            "\n",
            "\n",
            "Chunks:\n",
            "It\n",
            "Companies Email\n",
            "i\n",
            "india\n",
            "I\n",
            "mango\n",
            "\n",
            "\n",
            "POS_: ['PRON', 'VERB', 'SYM', 'NUM', 'NUM', 'CCONJ', 'PROPN', 'PROPN', 'AUX', 'X', 'CCONJ', 'PRON', 'VERB', 'ADP', 'PROPN', 'PUNCT', 'ADV', 'ADV', 'PRON', 'AUX', 'VERB', 'NOUN']\n",
            "tag_: ['PRP', 'VBD', '$', 'CD', 'CD', 'CC', 'NNPS', 'NNP', 'VBZ', 'ADD', 'CC', 'PRP', 'VBP', 'IN', 'NNP', ',', 'RB', 'RB', 'PRP', 'VBP', 'VBG', 'NN']\n",
            "tag_: ['pronoun, personal', 'verb, past tense', 'symbol, currency', 'cardinal number', 'cardinal number', 'conjunction, coordinating', 'noun, proper plural', 'noun, proper singular', 'verb, 3rd person singular present', 'email', 'conjunction, coordinating', 'pronoun, personal', 'verb, non-3rd person singular present', 'conjunction, subordinating or preposition', 'noun, proper singular', 'punctuation mark, comma', 'adverb', 'adverb', 'pronoun, personal', 'verb, non-3rd person singular present', 'verb, gerund or present participle', 'noun, singular or mass']\n",
            "lemma_: ['-PRON-', 'cost', '$', '5', 'million', 'and', 'Companies', 'Email', 'be', 'ansarijuned4all@gmail.com', 'and', 'i', 'live', 'in', 'india', ',', 'right', 'now', '-PRON-', 'be', 'eat', 'mango']\n",
            "is_stop: [True, False, False, False, False, True, False, False, True, False, True, True, False, True, False, False, False, True, True, True, False, False]\n",
            "is_alpha: [True, True, False, False, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True]\n",
            "is_punct: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False]\n",
            "like_num: [False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
            "like_email: [False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False]\n",
            "doc.ents Length: 4\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"It cost $5 million and Companies Email is ansarijuned4all@gmail.com and i live in india, right now I am eating mango\")\n",
        "print(\"Sentence Token:\", [sentense for sentense in doc.sents])\n",
        "# Sentence Token: [It cost $5 million and Companies Email is ansarijuned4all@gmail.com, and i live in india, , right now I am eating mango]\n",
        "print(\"Token:\", [token.text for token in doc])\n",
        "# Token: ['It', 'cost', '$', '5', 'million', 'and', 'Companies', 'Email', 'is', 'ansarijuned4all@gmail.com', 'and', 'i', 'live', 'in', 'india', ',', 'right', 'now', 'I', 'am', 'eating', 'mango']\n",
        "print(\"Token Index:\", [token.i for token in doc])\n",
        "# Token Index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
        "print(\"\\n\")\n",
        "print(\"Named Entity Recongnisation(NER):\")\n",
        "for token in doc.ents:\n",
        "  print(token.text,\":::Label(N E R):::\",token.label_,\"::: Explain:::\",spacy.explain(token.label_))\n",
        "\"\"\"\n",
        "Named Entity Recongnisation(NER):\n",
        "$5 million :::Label(N E R)::: MONEY ::: Explain::: Monetary values, including unit\n",
        "Companies Email :::Label(N E R)::: ORG ::: Explain::: Companies, agencies, institutions, etc.\n",
        "ansarijuned4all@gmail.com :::Label(N E R)::: ORG ::: Explain::: Companies, agencies, institutions, etc.\n",
        "india :::Label(N E R)::: GPE ::: Explain::: Countries, cities, states\n",
        "\"\"\"\n",
        "print(\"\\n\")\n",
        "print(\"POS_:\",[token.pos_ for token in doc])\n",
        "# POS_: ['PRON', 'VERB', 'SYM', 'NUM', 'NUM', 'CCONJ', 'PROPN', 'PROPN', 'AUX', 'X', 'CCONJ', 'PRON', 'VERB', 'ADP', 'PROPN', 'PUNCT', 'ADV', 'ADV', 'PRON', 'AUX', 'VERB', 'NOUN']\n",
        "print(\"tag_:\",[token.tag_ for token in doc])\n",
        "# tag_: ['PRP', 'VBD', '$', 'CD', 'CD', 'CC', 'NNPS', 'NNP', 'VBZ', 'ADD', 'CC', 'PRP', 'VBP', 'IN', 'NNP', ',', 'RB', 'RB', 'PRP', 'VBP', 'VBG', 'NN']\n",
        "print(\"tag_:\",[spacy.explain(token.tag_) for token in doc])\n",
        "# tag_: ['pronoun, personal', 'verb, past tense', 'symbol, currency', 'cardinal number', 'cardinal number', 'conjunction, coordinating', 'noun, proper plural', 'noun, proper singular', 'verb, 3rd person singular present', 'email', 'conjunction, coordinating', 'pronoun, personal', 'verb, non-3rd person singular present', 'conjunction, subordinating or preposition', 'noun, proper singular', 'punctuation mark, comma', 'adverb', 'adverb', 'pronoun, personal', 'verb, non-3rd person singular present', 'verb, gerund or present participle', 'noun, singular or mass']\n",
        "print(\"lemma_:\",[token.lemma_ for token in doc])\n",
        "# lemma_: ['-PRON-', 'cost', '$', '5', 'million', 'and', 'Companies', 'Email', 'be', 'ansarijuned4all@gmail.com', 'and', 'i', 'live', 'in', 'india', ',', 'right', 'now', '-PRON-', 'be', 'eat', 'mango']\n",
        "print(\"is_stop:\",[token.is_stop for token in doc])\n",
        "# is_stop: [True, False, False, False, False, True, False, False, True, False, True, True, False, True, False, False, False, True, True, True, False, False]\n",
        "print(\"is_alpha:\", [token.is_alpha for token in doc])\n",
        "# is_alpha: [True, True, False, False, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True]\n",
        "print(\"is_punct:\", [token.is_punct for token in doc])\n",
        "# is_punct: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False]\n",
        "print(\"like_num:\", [token.like_num for token in doc])\n",
        "# like_num: [False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
        "print(\"like_email:\", [token.like_email for token in doc])\n",
        "# like_email: [False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False]\n",
        "print(\"doc.ents Length:\",len(doc.ents))\n",
        "#doc.ents Length: 4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYT9xsrsIiio"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJKCRY_aJMt6",
        "outputId": "19febcca-a5ca-49ad-e22e-d99d2aa3d307"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run -----> run\n",
            "running -----> run\n",
            "ran -----> ran\n",
            "runs -----> run\n",
            "runner -----> runner\n",
            "easily -----> easili\n",
            "fairly -----> fairli\n"
          ]
        }
      ],
      "source": [
        "# Stemming is not a part of Spacy, it is part of NLTK\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "p_stemmer = PorterStemmer()\n",
        "words = ['run','running','ran','runs','runner','easily','fairly']\n",
        "for word in words:\n",
        "  print(word,\"----->\",p_stemmer.stem(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng4kJmgPBFLS"
      },
      "source": [
        "***Lemmatization***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YityK8iGLXjN",
        "outputId": "3d1f60f7-a4a2-4920-9c4b-5fcdf9536176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token: Tesla       POS_=>  PROPN     lemma_=>  Tesla      dep_=>  nsubj\n",
            "Token: is       POS_=>  AUX     lemma_=>  be      dep_=>  aux\n",
            "Token: looking       POS_=>  VERB     lemma_=>  look      dep_=>  ROOT\n",
            "Token: to       POS_=>  PART     lemma_=>  to      dep_=>  aux\n",
            "Token: buy       POS_=>  VERB     lemma_=>  buy      dep_=>  xcomp\n",
            "Token: U.S       POS_=>  PROPN     lemma_=>  U.S      dep_=>  npadvmod\n",
            "Token: based       POS_=>  VERB     lemma_=>  base      dep_=>  amod\n",
            "Token: startup       POS_=>  PROPN     lemma_=>  startup      dep_=>  compound\n",
            "Token: company       POS_=>  NOUN     lemma_=>  company      dep_=>  dobj\n",
            "Token: for       POS_=>  ADP     lemma_=>  for      dep_=>  prep\n",
            "Token: $       POS_=>  SYM     lemma_=>  $      dep_=>  quantmod\n",
            "Token: 6       POS_=>  NUM     lemma_=>  6      dep_=>  compound\n",
            "Token: million       POS_=>  NUM     lemma_=>  million      dep_=>  pobj\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(u'Tesla is looking to buy U.S based startup company for $6 million')\n",
        "for token in doc:\n",
        "  print(\"Token:\", token, \"      POS_=> \" ,token.pos_ , \"    lemma_=> \", token.lemma_, \"     dep_=> \", token.dep_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe6RgEEaYl1I"
      },
      "source": [
        "***Stop Words***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VqvxWFbYsVA"
      },
      "outputs": [],
      "source": [
        "#Import the English Language Class\n",
        "import spacy\n",
        "\n",
        "from spacy.lang.en import English\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "\n",
        "# import nltk\n",
        "# nltk.download(\"stopwords\")\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSvD0kZwatFH",
        "outputId": "5f9b2fe0-6cd5-4efd-9862-4ce804acdbe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'me', 'hundred', 'both', 'down', 'although', 'herself', 'during', 'somehow', 'there', 'who', 'regarding', 'almost', 'now', 'ten', 'these', 'he', 'everywhere', 'any', 'done', 'above', 'no', 'part', 'nobody', 'doing', '‘re', 'anyhow', 'either', 'wherein', 'an', 'on', 'however', 'could', 'whither', 'with', 'thereafter', 'ourselves', 'much', 'yet', 'whatever', 'used', \"'m\", 'myself', 'beside', 'eleven', 'further', 'whence', 'already', 'seem', 'five', 'mostly', 'top', 'whole', 'yourselves', 'can', 'and', 'herein', 'meanwhile', 'sometimes', 'their', 'most', 'beforehand', 'do', 'full', 'several', 'call', 'those', 'whereas', 'due', 'cannot', 'have', \"n't\", 'anywhere', 'twelve', 'whom', 'put', 'over', 'four', 'seemed', 'the', \"'d\", 'something', 'by', '’re', 'one', 'latterly', 'back', 'except', '’m', 'someone', 'your', 'move', 'as', 'becomes', 'became', 'its', 'of', 'onto', 'after', 'seems', 'anyway', 'than', 'few', 'say', 'below', 'whereupon', 'towards', 'enough', 'nevertheless', 'therein', 'namely', 'twenty', 'what', \"'re\", '’ll', 'her', 'hereupon', 'seeming', 'hers', 'too', 'upon', \"'ll\", 'therefore', 'whenever', 'throughout', 'serious', 'whose', 'see', 'though', 'n‘t', 'only', 're', 'more', '’d', 'thereupon', 'everything', 'front', 'two', 'wherever', 'each', 'around', 'but', 'him', 'did', 'show', 'still', 'thus', 'rather', 'various', \"'ve\", 'made', 'while', 'well', 'will', 'anything', 'thereby', 'perhaps', 'via', 'ca', 'again', 'neither', 'unless', 'very', 'once', 'give', 'into', 'had', 'were', 'third', 'if', 'along', 'being', 'go', 'former', 'might', 'should', 'six', 'somewhere', 'side', 'thence', 'using', 'anyone', 'because', 'every', 'nine', 'nowhere', 'between', 'own', 'been', 'they', 'are', 'first', 'next', 'fifteen', 'before', 'whether', 'from', 'why', 'afterwards', 'everyone', 'whoever', 'ours', 'yourself', 'it', 'i', 'bottom', 'nor', 'noone', 'some', 'which', 'at', 'hence', 'against', 'am', 'ever', 'may', 'please', 'really', 'sixty', 'less', 'about', 'that', 'so', 'off', 'beyond', 'through', '‘s', 'among', 'is', 'you', '‘ll', 'mine', 'all', '’ve', 'does', 'toward', '’s', 'himself', 'otherwise', 'under', 'another', \"'s\", 'least', 'to', 'together', 'becoming', 'here', 'up', 'often', 'us', '‘m', 'then', 'same', 'take', 'per', 'hereby', 'itself', 'else', 'until', 'hereafter', 'just', 'none', 'amongst', 'forty', 'for', 'name', 'behind', 'indeed', 'not', 'others', 'in', 'be', 'his', 'how', 'she', 'across', 'this', 'formerly', 'last', 'alone', 'many', 'yours', 'amount', 'has', 'without', 'elsewhere', 'fifty', 'always', 'quite', 'become', 'besides', 'empty', 'nothing', 'out', 'whereafter', 'we', 'even', 'when', '‘ve', '‘d', 'other', 'must', 'three', 'eight', 'make', 'keep', 'where', 'since', 'get', 'or', 'such', 'them', 'never', 'within', 'n’t', 'sometime', 'was', 'a', 'would', 'whereby', 'our', 'my', 'latter', 'moreover', 'themselves', 'also', 'thru'}\n"
          ]
        }
      ],
      "source": [
        "# list of stopwords in english language\n",
        "print(nlp.Defaults.stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-y3EYkBc6G4",
        "outputId": "6bfe077d-5712-4004-8cef-ed1a884ee218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "# how to check whether any word is stopwords or not\n",
        "print(nlp.vocab['via'].is_stop)\n",
        "print(nlp.vocab['india'].is_stop)\n",
        "print(nlp.vocab['testing'].is_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbHTdqBkd1tN",
        "outputId": "0e27f2c3-956e-4934-d02f-48eb19ab6559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# to add any non-stopword into stop word list\n",
        "nlp.Defaults.stop_words.add('testing')\n",
        "# set the stopword tag on the lexeme\n",
        "nlp.vocab['testing'].is_stop = True\n",
        "print(nlp.vocab['testing'].is_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Lat4WofeQyL",
        "outputId": "2044ccee-e627-4e03-c8ec-421ebbff44e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "# to remove any non-stopword into stop word list\n",
        "nlp.Defaults.stop_words.remove('testing')\n",
        "# remove the stopword tag on the lexeme\n",
        "nlp.vocab['testing'].is_stop = False\n",
        "print(nlp.vocab['testing'].is_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj8Lr0azfFkp",
        "outputId": "d1a3dbe8-5021-4927-ea3c-f5c50e89c44f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello : False\n",
            "World : False\n",
            ", : False\n",
            "Juned : False\n"
          ]
        }
      ],
      "source": [
        "# Filterout non-stopwords from the doc.\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "doc = nlp('Hello World, I am Juned')\n",
        "for word in doc:\n",
        "    if(str(word).lower() not in STOP_WORDS):\n",
        "      print(word,\":\",word.is_stop)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IOolhSPtVkd"
      },
      "source": [
        "**NER(Named Entity Recognision)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSL-qHCRyekw"
      },
      "outputs": [],
      "source": [
        "# we can also add out custom entity to NER\n",
        "doc = nlp(u\"It cost $5 millions and Companies Email is ansarijuned4all@gmail.com and i live in india, right now I am eating mango\")\n",
        "for token in doc.ents:\n",
        "  print(\"Entity:\",token.text)\n",
        "  print(\"Label:\",token.label_)\n",
        "  print(\"Explain:\",spacy.explain(token.label_))\n",
        "  print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir3QXoaBoDbC",
        "outputId": "82e33413-09cb-4977-d6b8-8dc0711380b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 22,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(doc.ents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "Nv0XoN3_ychM",
        "outputId": "12720279-e271-42b0-b9c7-b4f471d21e23"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">It cost $5 millions and Companies Email is ansarijuned4all@gmail.com and i live in india, right now I am eating mango</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from spacy import displacy\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyEPNdflVTjy",
        "outputId": "fb443d69-62e1-429a-85b2-2d96b6ba5e56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 24,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Add custom entity to NER\n",
        "doc = nlp(u\"Tesla to build a U.K Factory for $6 million.\")\n",
        "list(doc.ents)\n",
        "# here tesla should be identified as ORG but spacy could't so lets see how to add"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoEwjy90V6Ml",
        "outputId": "d8d09d62-289e-49a4-c0cf-f00aa593bd14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "383\n",
            "[Tesla]\n"
          ]
        }
      ],
      "source": [
        "from spacy.tokens import Span\n",
        "ORG = doc.vocab.strings[u\"ORG\"]\n",
        "print(ORG)\n",
        "new_entity = Span(doc,0,1,label=ORG)\n",
        "doc.ents = list(doc.ents) + [new_entity]\n",
        "print(list(doc.ents)) # Now Tesla is identified as ORG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnzCPK33DACL"
      },
      "source": [
        "**Chunks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "_w2k7FFpDJ9Z",
        "outputId": "90b9bb7d-4143-47c1-a5ce-f21727a125ca"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-00e308f7f006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Split into Chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdoc9\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu\"Autonomous cars shift insurance liability toward manufacturers.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc9\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoun_chunks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mnoun_chunks\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E029] noun_chunks requires the dependency parse, which requires a statistical model to be installed and loaded. For more info, see the documentation:\nhttps://spacy.io/usage/models"
          ]
        }
      ],
      "source": [
        "# Split into Chunks\n",
        "doc9 = nlp(u\"Autonomous cars shift insurance liability toward manufacturers.\")\n",
        "for chunk in doc9.noun_chunks:\n",
        "    print(chunk.text)\n",
        "\n",
        "print(\"\\n\")\n",
        "doc10 = nlp(u\"Red cars do not carry higher insurance rates.\")\n",
        "for chunk in doc10.noun_chunks:\n",
        "    print(chunk.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uchtaTAZRWzQ"
      },
      "source": [
        "**POS Tagging**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrpqipFJ0dYs"
      },
      "outputs": [],
      "source": [
        "doc = nlp(u\"the quick brown fox jumped over the lazy dog's back.\")\n",
        "for token in doc:\n",
        "  print(f\"{token.text:{10}} {token.pos_:{10}} {token.tag_:{10}} {spacy.explain(token.tag_)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ3esa-Lfxlx"
      },
      "source": [
        "**Feature Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98COtLGZf152"
      },
      "outputs": [],
      "source": [
        "# For Categorical Data\n",
        "#### 1. Label/Ordinal Encoding\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({\"Country\":[\"India\",\"Japan\",\"US\"],\"GDP\":[-23,10,15]})\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hhCXyUJixlq"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['Country'] = le.fit_transform(df['Country'])\n",
        "print(\"Label Encoded DataFrame\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJGpsQwDi4lc"
      },
      "outputs": [],
      "source": [
        "#### 2. One Hot Encoding\n",
        "dummies = pd.get_dummies(df['Country'],drop_first=True) # to avoid dummy variable trap remove once column\n",
        "df_dummies = pd.concat([df,dummies],axis='columns')\n",
        "df_dummies\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# one_hot_enc = OneHotEncoder()\n",
        "# one_hot_enc.fit_transform(df['Country'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoaKOrptWEAp"
      },
      "outputs": [],
      "source": [
        "#### 2. One Hot Enconding - with Keras\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "text = \"The Qucik brown fox jumped over the lazy dog.\"\n",
        "words = text_to_word_sequence(text)\n",
        "print(words)\n",
        "vocab_size = len(words)\n",
        "print(\"Vocab_size:\",vocab_size)\n",
        "result = one_hot(text,vocab_size)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfHqZifIcO6u"
      },
      "source": [
        "**CountVectorizer vs TfidfVectorizer**\n",
        "\n",
        "=> It counts word frequency\n",
        "\n",
        "=> TfidfVectorizer and CountVectorizer both are methods for converting text data into vectors as model can process only numerical data.\n",
        "\n",
        "=> In CountVectorizer we only count the number of times a word appears in the document which results in biasing in favour of most frequent words. this ends up in ignoring rare words which could have helped is in processing our data more efficiently.\n",
        "\n",
        "**Challgenes/Limitation in Count Vector**\n",
        "\n",
        "\n",
        "*   Increase in size\n",
        "*   Many 0's\n",
        "*   no meaningful information\n",
        "\n",
        "To overcome this , we use TfidfVectorizer .\n",
        "\n",
        "=> In TfidfVectorizer we consider overall document weightage of a word. It helps us in dealing with most frequent words. Using it we can penalize them. TfidfVectorizer weights the word counts by a measure of how often they appear in the documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "k1ByPIM5cSvM",
        "outputId": "13f27701-515a-4a9a-db75-ec075071d048"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chess</th>\n",
              "      <th>cricket</th>\n",
              "      <th>football</th>\n",
              "      <th>game</th>\n",
              "      <th>logic</th>\n",
              "      <th>strength</th>\n",
              "      <th>teach</th>\n",
              "      <th>teamwork</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.917227</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.180576</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.180576</td>\n",
              "      <td>0.305742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.608845</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.359594</td>\n",
              "      <td>0.608845</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.359594</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.608845</td>\n",
              "      <td>0.359594</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.608845</td>\n",
              "      <td>0.359594</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      chess   cricket  football  ...  strength     teach  teamwork\n",
              "0  0.000000  0.917227  0.000000  ...  0.000000  0.180576  0.305742\n",
              "1  0.608845  0.000000  0.000000  ...  0.000000  0.359594  0.000000\n",
              "2  0.000000  0.000000  0.608845  ...  0.608845  0.359594  0.000000\n",
              "\n",
              "[3 rows x 8 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "cv = CountVectorizer(stop_words='english')\n",
        "tv = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "messages=['Cricket Cricket Cricket Game teach us teamwork','Chess Game teach  us logic','football Game teach strength']\n",
        "tv_transformed_messages = tv.fit_transform(messages)\n",
        "pd.DataFrame(tv_transformed_messages.toarray(),columns = tv.get_feature_names())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbsQqq9xIgh1"
      },
      "outputs": [],
      "source": [
        "# N-Grams - to find similarities between words means relation between words maintained\n",
        "# it is used to auto complete like gmail\n",
        "cv = CountVectorizer(ngram_range=(2,2))\n",
        "messages=['Cricket Game teach us teamwork','Chess Game teach us logic','football Game teach strength']\n",
        "cv_transformed_messages = cv.fit_transform(messages)\n",
        "pd.DataFrame(cv_transformed_messages.toarray(),columns = cv.get_feature_names())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNVpZb1dgTND"
      },
      "outputs": [],
      "source": [
        "# tfidf vectorizer\n",
        "t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vG5o0nd1lfY-",
        "outputId": "bb671df8-3045-4da5-b1e4-706a600789ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=14, size=100, alpha=0.025)\n",
            "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
          ]
        }
      ],
      "source": [
        "# Word2vec : for each word, dimension are created which captures the meaning of the word.\n",
        "# skip-grams : predict context words from target word\n",
        "# cbow : predict target word from context words\n",
        "# define tokenized senences as training data\n",
        "tokenized_sentences = [['Hello','This','is','python','training','by','Aman'],\n",
        "             ['Hello','This','is','Java','training','by','Aman'],\n",
        "             ['Hello','This','is','Data Science','training','by','Unfold','Data','Science'],\n",
        "             ['Hello','This','is','programming','training','']]\n",
        "\n",
        "# training word2vec model\n",
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "mymodel_cbow = Word2Vec(tokenized_sentences, min_count=1, size = 100)\n",
        "mymodel_skipgram = Word2Vec(tokenized_sentences, min_count=1, size = 100,sg=1)\n",
        "# summarizing the loaded model\n",
        "print(mymodel_cbow)\n",
        "print(mymodel_skipgram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MdhR-82NULR"
      },
      "outputs": [],
      "source": [
        "# summarize vocabulary\n",
        "words = list(mymodel_cbow.wv.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxKf-ITzNbtA",
        "outputId": "35d245f3-8cfc-4697-e719-1a7cb2887dea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', 'This', 'is', 'python', 'training', 'by', 'Aman', 'Java', 'Data Science', 'Unfold', 'Data', 'Science', 'programming', '']\n"
          ]
        }
      ],
      "source": [
        "# summarize vocabulary\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhelekNlNdt1",
        "outputId": "e4be77e9-1a9a-4bb3-f661-9387ed5439f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-3.5017885e-03 -6.6767843e-04 -5.3375436e-04 -4.7164345e-03\n",
            " -2.3350744e-03  4.8523629e-03  1.7086894e-03  2.3700360e-03\n",
            "  3.1169606e-03 -3.5656586e-03  4.9215849e-03 -2.5933129e-03\n",
            "  1.1365833e-03 -4.0540900e-03  2.0094363e-03  8.8771677e-04\n",
            " -3.7866542e-03  1.5079991e-03  2.8396701e-03  3.5960509e-03\n",
            "  3.8333554e-03  4.8044329e-03 -1.9060089e-05 -4.5728385e-03\n",
            " -5.3578528e-04  4.3138801e-03  1.3313048e-03 -1.9220739e-03\n",
            " -8.5436093e-04 -1.0639599e-03  2.8188494e-03 -1.4518197e-03\n",
            "  2.2651383e-04 -1.7388945e-03 -6.3168831e-05  3.8615144e-03\n",
            " -1.1856877e-03 -4.4560218e-03  3.6558737e-03 -2.4500065e-03\n",
            "  1.3134355e-04  1.1331006e-03 -2.4853931e-03 -4.9010047e-04\n",
            "  1.4711343e-03  4.7697653e-03 -1.8651837e-03  3.0319807e-03\n",
            "  2.3098711e-03  3.7442821e-03  4.1510910e-03  1.0842441e-03\n",
            " -3.6488991e-04 -2.2211098e-03  2.2808870e-03  3.3509496e-03\n",
            "  1.3197073e-03 -1.8747314e-05  3.5300446e-03 -4.0952112e-03\n",
            "  1.3785980e-03  5.0904969e-04  1.5201821e-03  2.2016601e-04\n",
            "  2.4326751e-03  1.6952821e-03 -2.0926052e-03  2.2066722e-03\n",
            " -1.0146814e-03 -4.8644072e-03 -7.1507163e-04 -1.7853285e-03\n",
            "  6.9623051e-04  1.2107730e-03 -4.3345024e-04  3.3718278e-03\n",
            " -2.5000277e-03  3.1372639e-03  2.8654928e-03 -4.1355686e-03\n",
            "  3.4940194e-03  2.9700755e-03  2.9726345e-03 -4.8348645e-04\n",
            " -4.6209814e-03 -7.7733351e-04  4.2452939e-05  3.4294627e-03\n",
            "  1.0176145e-03  1.7311636e-03 -1.6829445e-03  1.0230310e-03\n",
            "  2.2474092e-06 -4.3647224e-03 -5.2990101e-04  3.7596163e-03\n",
            "  4.3053553e-03  1.5700982e-03 -7.1596965e-04 -1.0177292e-03]\n"
          ]
        }
      ],
      "source": [
        "# access word vector for one word \"training\"\n",
        "print(mymodel_cbow['Hello']) # here it wil print Hello word with 100 dimentions with different values for each dimention\n",
        "\n",
        "\"Bank of india\"  semantic search / context search\n",
        "\"Bank of River\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "hKyBp9wPr6Mj",
        "outputId": "5c738162-e979-4253-b67a-9e5b9faf7f37"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-17ffca17cab4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmymodel_cbow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m                 )\n\u001b[0;32m-> 1422\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \"\"\"\n\u001b[0;32m-> 1103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.__contains__() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'Hi' not in vocabulary\""
          ]
        }
      ],
      "source": [
        "print(mymodel_cbow['Hi'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "4Rgko2EpNobv",
        "outputId": "2c3592a9-585a-4cfc-eb30-0fc9c275cbaf"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c29bd4c49c29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#try finding most similar words for word \"Data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmymodel_cbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m                 )\n\u001b[0;32m-> 1422\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \"\"\"\n\u001b[0;32m-> 1397\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.wmdistance() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'Hi' not in vocabulary\""
          ]
        }
      ],
      "source": [
        "#try finding most similar words for word \"Data\"\n",
        "mymodel_cbow.most_similar(\"Hi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WhINE-mN4sL",
        "outputId": "e8e7aa24-0394-49e0-b7c3-36cf155bec5a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.17848499"
            ]
          },
          "execution_count": 7,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mymodel_cbow.similarity(\"python\",\"Java\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfdtEaSmQAzW"
      },
      "outputs": [],
      "source": [
        "#try finding most similar words for word \"Data\" using skip gram technique\n",
        "mymodel_skipgram.most_similar(\"Data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd4a7kXERlSH"
      },
      "outputs": [],
      "source": [
        "mymodel_skipgram.similarity(\"python\",\"Java\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZKpeRDkRrtQ"
      },
      "outputs": [],
      "source": [
        "# check words similarity using spacy\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "data = nlp(u\"cat lion pet\")\n",
        "for token1 in data:\n",
        "  for token2 in data:\n",
        "    print(token1,token2,token1.similarity(token2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZNpDdklcHpl"
      },
      "source": [
        "**Sentiment Analysis**\n",
        "\n",
        "VADER's `SentimentIntensityAnalyzer()` takes in a string and returns a dictionary of scores in each of four categories:\n",
        "* negative\n",
        "* neutral\n",
        "* positive\n",
        "* compound *(computed by normalizing the scores above, (i.e. aggregated score)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLYyVaXGZNRc"
      },
      "outputs": [],
      "source": [
        "#using nltk\n",
        "import nltk\n",
        "nltk.download(\"vader_lexicon\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFtA-KUFcqDj"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-BQRVEadOeG"
      },
      "outputs": [],
      "source": [
        "a = \"this is a good movie ever seen\"\n",
        "sia.polarity_scores(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJt6SQg1d-JU"
      },
      "outputs": [],
      "source": [
        "a = \"bad movie\"\n",
        "sia.polarity_scores(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H88mX3vmpc_U"
      },
      "source": [
        "# TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnYQxDm7ePb0"
      },
      "outputs": [],
      "source": [
        "# Spelling Correction\n",
        "#!pip install textblob\n",
        "#!python -m textblob.download_corpora\n",
        "from textblob import TextBlob\n",
        "x = \"Thankks forr watching it.\"\n",
        "TextBlob(x).correct()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O18C9Ym-ppnN"
      },
      "outputs": [],
      "source": [
        "# tokenization\n",
        "x = \"Thanks#for waching this video please like it\"\n",
        "tokens = TextBlob(x).words\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-Jclj5QsCLQ"
      },
      "outputs": [],
      "source": [
        "# Language Translation\n",
        "x = \"Hello India How are you?\"\n",
        "print(TextBlob(x).detect_language())\n",
        "print(TextBlob(x).translate(to = 'hi'))\n",
        "print(TextBlob(x).translate(to = 'gu'))\n",
        "print(TextBlob(x).translate(to = 'ur'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ajapdqr4svZz"
      },
      "outputs": [],
      "source": [
        "# Sentiment Analysis\n",
        "from textblob.sentiments import NaiveBayesAnalyzer\n",
        "x = \"we all stands together. we are gonna win this fight\"\n",
        "TextBlob(x, analyzer = NaiveBayesAnalyzer()).sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BsSCBkMPsX6"
      },
      "source": [
        "## Auto NLP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5spISUBfPxm2",
        "outputId": "53be485b-5343-41fd-ecb3-e74012cdea60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting autoviml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/5a/b27f0d9a7341ea18f1d8a861c8364acc98897afd1d837ea46e5e68399142/autoviml-0.1.677-py3-none-any.whl (121kB)\n",
            "\r\u001b[K     |██▊                             | 10kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 20kB 21.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 26.5MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 40kB 30.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 61kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 71kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 81kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 92kB 3.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 102kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 112kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from autoviml) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from autoviml) (0.11.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from autoviml) (4.6.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from autoviml) (1.1.5)\n",
            "Collecting imbalanced-learn>=0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/98/dc784205a7e3034e84d41ac4781660c67ad6327f2f5a80c568df31673d1c/imbalanced_learn-0.8.0-py3-none-any.whl (206kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from autoviml) (2019.12.20)\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 55.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from autoviml) (1.0.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from autoviml) (5.5.0)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (from autoviml) (0.15.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from autoviml) (3.2.5)\n",
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 46.9MB/s \n",
            "\u001b[?25hCollecting reg-resampler\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/fe/911c5f0ee409b18ffeff45cda6e7e7f2196fe604feaf6c4f3483ede46c27/reg_resampler-2.1.1-py3-none-any.whl\n",
            "Collecting xgboost>=1.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/57/bf5026701c384decd2b995eb39d86587a103ba4eb26f8a9b1811db0896d3/xgboost-1.3.3-py3-none-manylinux2010_x86_64.whl (157.5MB)\n",
            "\u001b[K     |████████████████████████████████| 157.5MB 77kB/s \n",
            "\u001b[?25hCollecting scikit-learn>=0.23.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 1.4MB/s \n",
            "\u001b[?25hCollecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/3b/bb419654adcf7efff42ed8a3f84e50c8f236424b7ed1cc8ccd290852e003/catboost-0.24.4-cp37-none-manylinux1_x86_64.whl (65.7MB)\n",
            "\u001b[K     |████████████████████████████████| 65.7MB 1.5MB/s eta 0:00:01"
          ]
        }
      ],
      "source": [
        "!pip install autoviml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "823wfrc5ROh6"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POEgaRXXbW6-"
      },
      "outputs": [],
      "source": [
        "dataset, info = tfds.load('amazon_us_reviews/Personal_Care_Appliances_v1_00', with_info=True, batch_size=-1)\n",
        "train_dataset = dataset['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9FiFe0DuRLd"
      },
      "outputs": [],
      "source": [
        "info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVTn8Dz6uYrQ"
      },
      "outputs": [],
      "source": [
        "dataset=tfds.as_numpy(train_dataset)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gZ5xaVwudY2"
      },
      "outputs": [],
      "source": [
        "helpful_votes=dataset['data']['helpful_votes']\n",
        "review_headline=dataset['data']['review_headline']\n",
        "review_body=dataset['data']['review_body']\n",
        "rating=dataset['data']['star_rating']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7YmxAvUuqLX"
      },
      "outputs": [],
      "source": [
        "reviews_df=pd.DataFrame(np.hstack((helpful_votes[:,None],review_headline[:,None],review_body[:,None],rating[:,None])),columns=['votes','headline','reviews','rating'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvdpUSIeusYM"
      },
      "outputs": [],
      "source": [
        "convert_dict = {'votes': int,\n",
        "                'headline': str,\n",
        "                'reviews': str,\n",
        "                'rating': int\n",
        "               }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EErrekCuwGL"
      },
      "outputs": [],
      "source": [
        "reviews_df = reviews_df.astype(convert_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCg2Dq1jux9L"
      },
      "outputs": [],
      "source": [
        "reviews_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CjYO2AVuzOL"
      },
      "outputs": [],
      "source": [
        "reviews_df[\"target\"] = reviews_df[\"rating\"].apply(lambda x: 1 if x>= 4 else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yh91BxCDu5Nl"
      },
      "outputs": [],
      "source": [
        "reviews_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfY6n59yu7TG"
      },
      "outputs": [],
      "source": [
        "reviews_df.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPT4-e7wvG_k"
      },
      "outputs": [],
      "source": [
        "reviews_df[\"target\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af-4k-8qvJ8v"
      },
      "outputs": [],
      "source": [
        "reviews_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9KVV4vrvL0W"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(reviews_df, test_size=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQaL4QtHvPil"
      },
      "outputs": [],
      "source": [
        "from autoviml.Auto_NLP import Auto_NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4qYC2fVvRUi"
      },
      "outputs": [],
      "source": [
        "nlp_column = 'reviews'\n",
        "target = 'target'\n",
        "train_nlp, test_nlp, nlp_transformer, preds = Auto_NLP(\n",
        "                nlp_column, train, test, target, score_type='balanced_accuracy',\n",
        "                modeltype='Classification',top_num_features=50, verbose=2,\n",
        "                build_model=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LpGMC4JvV1f"
      },
      "outputs": [],
      "source": [
        "nlp_transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0mR9kvLvcYU"
      },
      "outputs": [],
      "source": [
        "nlp_transformer.predict(test[nlp_column])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iOzO4qCveVm"
      },
      "outputs": [],
      "source": [
        "nlp_transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MIEwIuovglc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm3gLjQjF8tW"
      },
      "source": [
        "# BERT Vs Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoIct3KvGBet",
        "outputId": "72217456-aacb-454e-a26c-c0afbda2626c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flair\n",
            "  Downloading flair-0.12.2-py3-none-any.whl (373 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.1/373.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from flair) (2.8.2)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from flair) (2.0.1+cu118)\n",
            "Requirement already satisfied: gensim>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.3.1)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.66.1)\n",
            "Collecting segtok>=1.5.7 (from flair)\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from flair) (3.7.1)\n",
            "Collecting mpld3==0.3 (from flair)\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.5/788.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from flair) (1.2.2)\n",
            "Collecting sqlitedict>=1.6.0 (from flair)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated>=1.2.4 (from flair)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: hyperopt>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from flair) (0.2.7)\n",
            "Collecting boto3 (from flair)\n",
            "  Downloading boto3-1.28.32-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers[sentencepiece]>=4.18.0 (from flair)\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bpemb>=0.3.2 (from flair)\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from flair) (2023.6.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from flair) (0.9.0)\n",
            "Collecting langdetect (from flair)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from flair) (4.9.3)\n",
            "Collecting ftfy (from flair)\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting janome (from flair)\n",
            "  Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gdown==4.4.0 (from flair)\n",
            "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub>=0.10.0 (from flair)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting conllu>=4.0 (from flair)\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from flair) (10.1.0)\n",
            "Collecting wikipedia-api (from flair)\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Collecting pptree (from flair)\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch-revgrad (from flair)\n",
            "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
            "Collecting transformer-smaller-training-vocab>=0.2.1 (from flair)\n",
            "  Downloading transformer_smaller_training_vocab-0.3.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown==4.4.0->flair) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown==4.4.0->flair) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown==4.4.0->flair) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown==4.4.0->flair) (4.11.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair) (1.23.5)\n",
            "Collecting sentencepiece (from bpemb>=0.3.2->flair)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.4->flair) (1.14.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim>=3.8.0->flair) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=3.8.0->flair) (6.3.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (2023.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (23.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair) (3.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair) (0.18.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair) (0.10.9.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (3.1.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->flair) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->flair) (3.2.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.8,>=1.5.0->flair) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.8,>=1.5.0->flair) (16.0.6)\n",
            "Collecting torch!=1.8,>=1.5.0 (from flair)\n",
            "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch!=1.8,>=1.5.0->flair)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch!=1.8,>=1.5.0->flair)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch!=1.8,>=1.5.0->flair)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch!=1.8,>=1.5.0->flair)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch!=1.8,>=1.5.0->flair)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch!=1.8,>=1.5.0->flair)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch!=1.8,>=1.5.0->flair)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch!=1.8,>=1.5.0->flair)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch!=1.8,>=1.5.0->flair)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch!=1.8,>=1.5.0->flair)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch!=1.8,>=1.5.0->flair)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch!=1.8,>=1.5.0->flair) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch!=1.8,>=1.5.0->flair) (0.41.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers[sentencepiece]>=4.18.0->flair)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[sentencepiece]>=4.18.0->flair)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.18.0->flair) (3.20.3)\n",
            "Collecting botocore<1.32.0,>=1.31.32 (from boto3->flair)\n",
            "  Downloading botocore-1.31.32-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->flair)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->flair)\n",
            "  Downloading s3transfer-0.6.2-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->flair) (0.2.6)\n",
            "Collecting urllib3<1.27,>=1.25.4 (from botocore<1.32.0,>=1.31.32->boto3->flair)\n",
            "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.20.3 (from transformers[sentencepiece]>=4.18.0->flair)\n",
            "  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown==4.4.0->flair) (2.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.8,>=1.5.0->flair) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.4.0->flair) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.4.0->flair) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.4.0->flair) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.4.0->flair) (1.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.5.0->flair) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[sentencepiece]>=4.18.0->flair) (5.9.5)\n",
            "Building wheels for collected packages: gdown, mpld3, sqlitedict, langdetect, pptree\n",
            "  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14758 sha256=d252f5c95b22439f91ed43c9b216ee7146ea292ccc2d0113b59633c497fa7af5\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/0b/3f/6ddf67a417a5b400b213b0bb772a50276c199a386b12c06bfc\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116686 sha256=5b39c62a013b0c599141c952156dda4173fa697e834d16caaf6acc6727f7a7aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/92/f7/45d9aac5dcfb1c2a1761a272365599cc7ba1050ce211a3fd9a\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=a51f9c655bf04833099f1f57b0488833f002120415989545f1d64910e466b831\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=374f1348077115b721c5da4685424fb9927df111141d8bfc12e558de9dd62eaf\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4608 sha256=74c1019545ae5dd18d3b7ece10758cc2408243a8ba29551f1f7bb39f285e8813\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/b6/0e/6f26eb9e6eb53ff2107a7888d72b5a6a597593956113037828\n",
            "Successfully built gdown mpld3 sqlitedict langdetect pptree\n",
            "Installing collected packages: tokenizers, sqlitedict, sentencepiece, safetensors, pptree, mpld3, janome, urllib3, segtok, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, langdetect, jmespath, ftfy, deprecated, conllu, nvidia-cusolver-cu11, nvidia-cudnn-cu11, botocore, wikipedia-api, s3transfer, huggingface-hub, bpemb, transformers, gdown, boto3, torch, accelerate, transformer-smaller-training-vocab, pytorch-revgrad, flair\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.4\n",
            "    Uninstalling urllib3-2.0.4:\n",
            "      Successfully uninstalled urllib3-2.0.4\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.6.6\n",
            "    Uninstalling gdown-4.6.6:\n",
            "      Successfully uninstalled gdown-4.6.6\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.22.0 boto3-1.28.32 botocore-1.31.32 bpemb-0.3.4 conllu-4.5.3 deprecated-1.2.14 flair-0.12.2 ftfy-6.1.1 gdown-4.4.0 huggingface-hub-0.16.4 janome-0.5.0 jmespath-1.0.1 langdetect-1.0.9 mpld3-0.3 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pptree-3.1 pytorch-revgrad-0.2.0 s3transfer-0.6.2 safetensors-0.3.2 segtok-1.5.11 sentencepiece-0.1.99 sqlitedict-2.1.0 tokenizers-0.13.3 torch-2.0.0 transformer-smaller-training-vocab-0.3.1 transformers-4.32.0 urllib3-1.26.16 wikipedia-api-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install flair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-nJrzL1GDlY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from flair.embeddings import WordEmbeddings\n",
        "from flair.embeddings import TransformerWordEmbeddings\n",
        "from flair.data import Sentence\n",
        "from scipy.spatial import distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k70PawDPGHSZ",
        "outputId": "fd1c3bc4-6388-4d84-c480-ba7cae7e4b59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021-02-15 14:30:19,206 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmpkkygv79p\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 160000128/160000128 [00:14<00:00, 11072518.26B/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021-02-15 14:30:34,186 copying /tmp/tmpkkygv79p to cache at /root/.flair/embeddings/glove.gensim.vectors.npy\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021-02-15 14:30:34,483 removing temp file /tmp/tmpkkygv79p\n",
            "2021-02-15 14:30:35,827 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim not found in cache, downloading to /tmp/tmpr958f413\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21494764/21494764 [00:02<00:00, 7941329.81B/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021-02-15 14:30:39,034 copying /tmp/tmpr958f413 to cache at /root/.flair/embeddings/glove.gensim\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021-02-15 14:30:39,066 removing temp file /tmp/tmpr958f413\n"
          ]
        }
      ],
      "source": [
        "glove_embedding = WordEmbeddings('glove')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lA_MEukrGNzD"
      },
      "source": [
        "Sentence1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhjBahImGJMP"
      },
      "outputs": [],
      "source": [
        "sentence_1 = Sentence('apple released iphone 12 pro max in 2020')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnR3xSmAGR4P",
        "outputId": "4497de10-e39a-410d-ce34-1b96312b58ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Sentence: \"apple released iphone 12 pro max in 2020\"   [− Tokens: 8]]"
            ]
          },
          "execution_count": 5,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "glove_embedding.embed(sentence_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vir9oBJ2GTWm",
        "outputId": "300832b2-470c-49a6-ba6c-b9bb6fb1b5c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token: 1 apple\n",
            "tensor([-0.5985, -0.4632,  0.1300, -0.0196,  0.4603, -0.3018,  0.8977, -0.6563,\n",
            "         0.6686, -0.4916,  0.0376, -0.0509,  0.6451, -0.5388, -0.3765, -0.0431,\n",
            "         0.5138,  0.1778,  0.2860,  0.9206, -0.4935, -0.4858,  0.6132,  0.7821,\n",
            "         0.1925,  0.9123, -0.0556, -0.1251, -0.6569,  0.0686,  0.5563,  1.6110,\n",
            "        -0.0074, -0.4888,  0.4549,  0.9610, -0.0634,  0.1743,  0.9814, -1.3125,\n",
            "        -0.1580, -0.5430, -0.1389, -0.2615, -0.3691,  0.2684, -0.2438, -0.1948,\n",
            "         0.6258, -0.7377,  0.3835, -0.7500, -0.3905,  0.0915, -0.3659, -1.4715,\n",
            "        -0.4523,  0.2256,  1.1412, -0.3853, -0.0672,  0.5729, -0.3919,  0.3130,\n",
            "        -0.2923, -0.9616,  0.1515, -0.2166,  0.2510,  0.0970,  0.2843,  1.4296,\n",
            "        -0.5056, -0.5137, -0.4722,  0.3204,  0.0231,  0.2262, -0.0972,  0.8213,\n",
            "         0.9260, -1.0086, -0.3864,  0.8641, -1.2060, -0.2853,  0.2265, -0.3877,\n",
            "         0.4088,  0.5930,  0.3077,  0.8380, -0.6366, -0.4464, -0.4341, -0.7936,\n",
            "        -0.2867, -0.0344,  1.3431,  0.3490])\n",
            "\n",
            "\n",
            "Token: 2 released\n",
            "tensor([-0.0451, -0.7260,  0.6271, -0.8506,  1.2913,  0.1556,  0.5372,  0.2967,\n",
            "         0.0272, -0.2156,  0.7911,  0.1594,  0.5834,  0.1422,  0.2127,  0.5273,\n",
            "         0.8467, -0.2552, -0.2266,  0.0687,  0.8454, -1.0389,  0.0572,  0.7512,\n",
            "        -0.4233,  0.3479,  0.3719, -0.0245,  0.4516,  1.0017,  0.7148,  0.0396,\n",
            "        -0.7914,  1.1237,  0.0218,  0.5682, -0.1066,  0.4893, -0.1981, -0.9888,\n",
            "        -0.2954,  0.2199, -0.1736,  1.0455,  0.3021, -0.5135, -0.4400, -0.8309,\n",
            "         0.4581, -0.4946,  0.3384, -0.4309,  0.0789,  0.8418, -0.3683, -2.4302,\n",
            "        -0.5795, -0.0524,  1.2644,  0.8816, -0.0311,  0.9024, -1.3107, -0.7962,\n",
            "         0.9831,  0.0515, -0.0143,  0.6118,  0.2169, -0.0897,  0.1589,  0.5384,\n",
            "        -0.3691,  0.0449, -0.1022,  0.9028, -0.0959, -0.0254, -0.9284, -0.5310,\n",
            "         0.4139, -0.0872, -0.0959,  0.5326, -1.1456, -0.2831,  0.6678, -0.5193,\n",
            "        -0.3507,  0.6526, -0.1945, -0.3041, -0.6515,  0.2619, -0.0246,  0.2302,\n",
            "        -0.3672,  0.1771,  0.5437, -0.9623])\n",
            "\n",
            "\n",
            "Token: 3 iphone\n",
            "tensor([-0.3609, -0.5975,  0.5235,  0.2344,  0.3229, -0.5134,  0.4237, -0.5790,\n",
            "         0.6805,  0.3536,  0.6597, -0.0775,  0.0842, -0.7555,  0.5229,  0.0685,\n",
            "        -0.3928,  1.1090,  0.7667,  0.6545, -0.0071, -1.1910,  0.8565,  1.1666,\n",
            "         1.1307,  0.9571,  0.1755,  0.2592, -0.6803, -0.1571,  0.3787,  1.7457,\n",
            "         0.1398,  0.3117,  1.1883,  0.9100, -0.9956, -0.2546,  0.6254, -0.6041,\n",
            "         0.5139, -0.1973,  0.3690, -0.0774, -0.6047,  0.0814,  0.5492, -0.2851,\n",
            "         0.8859,  0.2887,  0.1664,  0.3513,  0.2691, -0.2735, -0.3812, -0.5627,\n",
            "        -0.7775, -0.2434,  0.8626, -0.2992, -0.1599,  0.5551, -0.3766,  0.3375,\n",
            "         0.0084,  0.3258, -0.2169, -0.4835,  0.4285, -0.6070,  0.7929,  1.0375,\n",
            "        -0.6710, -0.9116, -0.4302,  0.7993,  0.0236, -0.0459, -0.2122, -0.4890,\n",
            "         0.4448, -1.3734,  0.2017,  0.5503, -1.0103, -0.0994,  0.4866, -0.0043,\n",
            "        -0.4854,  0.1720,  0.2872,  1.1088,  0.4544, -0.8195,  0.4331, -0.3293,\n",
            "        -0.3243,  0.2213,  1.4631,  0.2227])\n",
            "\n",
            "\n",
            "Token: 4 12\n",
            "tensor([ 0.0730,  0.1813,  0.0204, -0.2854,  0.1390,  0.1947,  0.7946,  0.5717,\n",
            "        -0.7715,  0.0392,  1.1718, -0.2051, -0.2833,  0.3860,  0.6874,  0.0520,\n",
            "        -0.0109,  0.1280, -1.0185,  0.7395,  0.7109, -0.1611,  0.5491,  0.6137,\n",
            "         0.3223, -0.4123,  0.3328, -0.0841,  0.2424,  0.3612,  0.0668,  0.1613,\n",
            "         0.1812, -0.7899, -0.4400, -0.0025,  0.0969,  0.3055, -0.4306,  0.6801,\n",
            "        -0.0809, -0.7408,  0.7272, -0.3813,  0.3725, -0.4571, -0.3741, -0.8136,\n",
            "        -0.0643, -0.1498, -0.6084, -0.7011, -0.3335,  0.9525, -0.5965, -2.3970,\n",
            "        -0.1315, -0.6259,  1.7407,  0.9491, -0.5584,  0.7337, -0.6383,  0.1429,\n",
            "         0.0523,  0.5167, -0.4722,  0.4084,  0.1880,  0.3140, -0.2562,  0.0069,\n",
            "        -0.0765,  0.5165, -0.0218,  0.1769,  0.1347,  0.1100, -0.7226, -0.3386,\n",
            "         0.7269, -0.0877, -0.3665,  0.1536, -0.4635, -0.4650,  0.3513, -0.1610,\n",
            "         0.2442,  0.5036, -0.1055,  0.4940, -0.2373, -0.1125, -1.2820,  0.4075,\n",
            "        -0.3709, -0.0305,  0.4792, -0.5814])\n",
            "\n",
            "\n",
            "Token: 5 pro\n",
            "tensor([-0.2332,  0.1988, -0.2101, -0.5201,  0.0146,  1.0865,  0.3593, -0.5337,\n",
            "        -0.2569,  0.4887, -0.0698,  0.0261,  0.5549, -0.4796, -0.4538,  0.3965,\n",
            "         0.0355, -1.0047, -0.1908,  0.4501,  0.6563,  0.0932,  0.0396,  0.1032,\n",
            "         0.2977, -0.2558, -0.3840, -0.7571,  0.9946,  0.9950,  0.2262,  1.1416,\n",
            "         0.0645,  0.2327,  0.7881, -0.2046, -0.0235,  0.6137, -0.7019, -0.2946,\n",
            "        -0.6030, -0.2245,  1.0954, -0.4636, -0.3176, -0.9000,  0.2299, -0.3905,\n",
            "        -0.0554, -0.5276, -0.3356,  0.1659, -0.2207,  0.0882,  0.5056, -1.6934,\n",
            "         0.5285,  0.6024,  1.6912,  0.3494, -0.7740, -0.5321, -0.5961, -0.0379,\n",
            "         0.2280,  0.1600,  0.5961,  0.0467,  0.0882,  0.5184,  1.0067, -0.2359,\n",
            "        -0.8654,  0.0373, -0.8648, -0.3432,  0.3989,  0.5280, -0.3100, -0.5621,\n",
            "         1.2702, -0.5630, -0.1432, -0.3489, -0.3617,  0.0104, -0.1854,  0.2372,\n",
            "         0.5165, -0.3110,  0.1728,  0.3704, -0.8039,  0.9957, -0.3425, -0.3929,\n",
            "        -0.7508,  0.7523,  1.3898,  0.4525])\n",
            "\n",
            "\n",
            "Token: 6 max\n",
            "tensor([ 0.3356,  0.1247,  0.2099, -0.0868,  0.2193, -0.6803,  0.0619,  0.2419,\n",
            "        -0.5426, -0.5727,  0.2655, -0.1465, -1.0335, -0.1684, -0.0746,  0.1714,\n",
            "         0.6827, -0.1196, -0.4984,  0.0190,  0.2460,  0.1463,  0.8977,  0.1802,\n",
            "         0.2900,  0.1599,  0.1820, -0.3310,  0.6753,  0.5950, -0.3450,  0.6775,\n",
            "        -0.3929,  0.1636, -0.4901,  0.2833,  0.1133,  0.0250,  0.1972,  0.2408,\n",
            "         0.2869, -0.1621, -0.1463,  0.0591, -1.0204,  0.1693, -0.3522, -0.0614,\n",
            "         0.3289,  0.8992,  0.4285, -0.0598, -0.3730,  0.4329,  0.3592, -1.8269,\n",
            "        -0.4295,  0.9414, -0.3002,  0.2843,  1.0934,  0.2972, -0.6670,  0.3720,\n",
            "         1.3223, -0.1351,  0.4574,  0.8587,  0.7930,  0.4599,  0.1624, -0.8505,\n",
            "        -0.4849,  0.1712,  0.2690,  0.3976,  0.7205, -0.9071,  0.2739,  0.2051,\n",
            "        -0.0022, -0.1362, -0.3767, -0.5283, -0.5826,  0.2282, -0.2979, -0.2571,\n",
            "        -0.2076, -0.4404,  0.5630, -0.1047,  0.6858,  0.6947,  0.1091, -0.2399,\n",
            "         0.0042, -0.0966,  0.2621, -0.8044])\n",
            "\n",
            "\n",
            "Token: 7 in\n",
            "tensor([ 0.0857, -0.2220,  0.1657,  0.1337,  0.3824,  0.3540,  0.0129,  0.2246,\n",
            "        -0.4382,  0.5016, -0.3587, -0.3498,  0.0552,  0.6965, -0.1796,  0.0679,\n",
            "         0.3910,  0.1604, -0.2664, -0.2114,  0.5370,  0.4938,  0.9366,  0.6690,\n",
            "         0.2179, -0.4664,  0.2238, -0.3620, -0.1766,  0.1748, -0.2037,  0.1393,\n",
            "         0.0198, -0.1041, -0.2024,  0.5500, -0.1546,  0.9865, -0.2686, -0.2909,\n",
            "        -0.3287, -0.3419, -0.1694, -0.4200, -0.0467, -0.1633,  0.7082, -0.7491,\n",
            "        -0.0916, -0.9618, -0.1975,  0.1028,  0.5522,  1.3816, -0.6564, -3.2502,\n",
            "        -0.3156, -1.2055,  1.7709,  0.4026, -0.7983,  1.1597, -0.3304,  0.3138,\n",
            "         0.7739,  0.2260,  0.5247, -0.0341,  0.3205,  0.0799,  0.1775, -0.4943,\n",
            "        -0.7005, -0.4457,  0.1724,  0.2028,  0.0233, -0.2068, -1.0158,  0.1832,\n",
            "         0.5675,  0.3182, -0.6501,  0.6828, -0.8658, -0.0594, -0.2926, -0.5567,\n",
            "        -0.3471, -0.3289,  0.4022, -0.1275, -0.2023,  0.8737, -0.5450,  0.7921,\n",
            "        -0.2069, -0.0743,  0.7581, -0.3424])\n",
            "\n",
            "\n",
            "Token: 8 2020\n",
            "tensor([-0.1241,  0.3447,  0.9852, -0.1853,  0.3933, -1.3096, -0.0987,  0.6483,\n",
            "         0.1348,  1.0135,  0.4145, -0.4029,  0.1575, -1.1746,  0.3356, -1.6206,\n",
            "        -0.4004,  1.0787, -0.4370, -1.0824, -0.4016, -0.3710, -0.4385,  0.8210,\n",
            "        -0.1609,  0.4084,  0.1666, -0.0382,  0.4597, -0.0112, -0.5954,  0.4539,\n",
            "        -0.2334, -0.8493,  0.4425,  0.2576,  0.0797, -0.2632, -0.8782,  0.1784,\n",
            "        -0.5862,  0.0843, -0.3571,  0.9713, -0.0594,  0.0907,  0.1479, -0.6011,\n",
            "        -1.4061, -0.1069, -0.6418, -0.6196, -0.6186,  0.8040, -0.5495, -1.1144,\n",
            "         0.1015,  0.2140,  0.5193, -0.5009, -0.1563,  0.1541, -0.6118, -0.4091,\n",
            "         0.2161,  0.3469, -1.0338, -0.7153,  1.1187, -1.1841,  0.8466, -0.3190,\n",
            "        -0.4022,  0.0755, -0.4686, -0.5858,  0.6357, -0.5537, -0.7745, -0.5577,\n",
            "         0.4393,  0.4853, -0.3689, -0.3911,  0.0748,  0.6911,  0.4363, -0.1206,\n",
            "        -0.1954,  0.8056,  0.5050,  0.2136,  0.1071, -0.5304, -1.1339,  0.2752,\n",
            "         0.1600,  0.0397, -0.0084, -1.0729])\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for token in sentence_1:\n",
        "    print(token)\n",
        "    print(token.embedding)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "girvL57uGVGp",
        "outputId": "11a092ef-bddf-46f3-a644-34a3b7864eb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Token: 1 apple"
            ]
          },
          "execution_count": 7,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_1[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AFhExK5GuJN",
        "outputId": "039eff31-5263-48af-957a-b5e38ff508a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.5985, -0.4632,  0.1300, -0.0196,  0.4603, -0.3018,  0.8977, -0.6563,\n",
              "         0.6686, -0.4916,  0.0376, -0.0509,  0.6451, -0.5388, -0.3765, -0.0431,\n",
              "         0.5138,  0.1778,  0.2860,  0.9206, -0.4935, -0.4858,  0.6132,  0.7821,\n",
              "         0.1925,  0.9123, -0.0556, -0.1251, -0.6569,  0.0686,  0.5563,  1.6110,\n",
              "        -0.0074, -0.4888,  0.4549,  0.9610, -0.0634,  0.1743,  0.9814, -1.3125,\n",
              "        -0.1580, -0.5430, -0.1389, -0.2615, -0.3691,  0.2684, -0.2438, -0.1948,\n",
              "         0.6258, -0.7377,  0.3835, -0.7500, -0.3905,  0.0915, -0.3659, -1.4715,\n",
              "        -0.4523,  0.2256,  1.1412, -0.3853, -0.0672,  0.5729, -0.3919,  0.3130,\n",
              "        -0.2923, -0.9616,  0.1515, -0.2166,  0.2510,  0.0970,  0.2843,  1.4296,\n",
              "        -0.5056, -0.5137, -0.4722,  0.3204,  0.0231,  0.2262, -0.0972,  0.8213,\n",
              "         0.9260, -1.0086, -0.3864,  0.8641, -1.2060, -0.2853,  0.2265, -0.3877,\n",
              "         0.4088,  0.5930,  0.3077,  0.8380, -0.6366, -0.4464, -0.4341, -0.7936,\n",
              "        -0.2867, -0.0344,  1.3431,  0.3490])"
            ]
          },
          "execution_count": 8,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_1[0].embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Jtot5nTGvSe",
        "outputId": "0b626d21-2b18-4e2d-a8a2-b6a8a2ee0fb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([100])"
            ]
          },
          "execution_count": 9,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_1[0].embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2_BEvd8G1_Q"
      },
      "source": [
        "Sentence 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0S5J6fIaGyfh"
      },
      "outputs": [],
      "source": [
        "sentence_2 = Sentence('an apple a day keeps the doctor away')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q9FpagrG6Kc",
        "outputId": "98af5a90-a32d-490b-c57d-c03f16d6a2b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Sentence: \"an apple a day keeps the doctor away\"   [− Tokens: 8]]"
            ]
          },
          "execution_count": 12,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "glove_embedding.embed(sentence_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNCTZyAeG6tN",
        "outputId": "1fe48791-ece5-4f8c-9d16-a51b59a5d5d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token: 1 an\n",
            "tensor([-0.4214, -0.1880,  0.4624, -0.1761,  0.3621,  0.3670,  0.2792,  0.1463,\n",
            "        -0.0542,  0.4583,  0.0654, -0.3372,  0.0675, -0.3632,  0.5030, -0.0104,\n",
            "         0.7283, -0.1756, -0.3400,  0.0729,  0.6448, -0.2391,  0.3838,  0.1386,\n",
            "         1.0994, -0.2488, -0.1508, -0.4874, -0.2304,  0.0648, -0.7018,  0.8265,\n",
            "         0.0613,  0.1853, -0.3016, -0.0222,  0.3430,  0.8033,  0.1714,  0.1546,\n",
            "        -0.5076,  0.3957,  0.0543, -0.5308,  0.4825,  0.0862,  0.5958, -0.2238,\n",
            "        -0.3955, -0.7304, -0.1028, -0.3917,  1.2290,  1.2129, -1.0365, -3.4971,\n",
            "         0.1092, -1.0084,  1.9998,  0.7964,  0.3881,  0.4375,  0.0852,  0.3855,\n",
            "         0.6199, -1.0320,  0.7012, -0.2246,  0.0794,  0.0913, -0.2120, -0.5543,\n",
            "        -0.0534, -0.8020,  0.4680, -0.0501, -0.5742, -0.0848, -1.7227, -0.9429,\n",
            "         0.9867,  0.3121, -0.3774,  0.0687, -0.7784, -0.2849,  0.8105,  0.4660,\n",
            "        -0.1186, -0.9341,  0.3372,  0.0379, -0.1827, -0.0199,  0.2049, -0.4772,\n",
            "        -0.4925, -0.5652,  0.7256, -0.1591])\n",
            "\n",
            "\n",
            "Token: 2 apple\n",
            "tensor([-0.5985, -0.4632,  0.1300, -0.0196,  0.4603, -0.3018,  0.8977, -0.6563,\n",
            "         0.6686, -0.4916,  0.0376, -0.0509,  0.6451, -0.5388, -0.3765, -0.0431,\n",
            "         0.5138,  0.1778,  0.2860,  0.9206, -0.4935, -0.4858,  0.6132,  0.7821,\n",
            "         0.1925,  0.9123, -0.0556, -0.1251, -0.6569,  0.0686,  0.5563,  1.6110,\n",
            "        -0.0074, -0.4888,  0.4549,  0.9610, -0.0634,  0.1743,  0.9814, -1.3125,\n",
            "        -0.1580, -0.5430, -0.1389, -0.2615, -0.3691,  0.2684, -0.2438, -0.1948,\n",
            "         0.6258, -0.7377,  0.3835, -0.7500, -0.3905,  0.0915, -0.3659, -1.4715,\n",
            "        -0.4523,  0.2256,  1.1412, -0.3853, -0.0672,  0.5729, -0.3919,  0.3130,\n",
            "        -0.2923, -0.9616,  0.1515, -0.2166,  0.2510,  0.0970,  0.2843,  1.4296,\n",
            "        -0.5056, -0.5137, -0.4722,  0.3204,  0.0231,  0.2262, -0.0972,  0.8213,\n",
            "         0.9260, -1.0086, -0.3864,  0.8641, -1.2060, -0.2853,  0.2265, -0.3877,\n",
            "         0.4088,  0.5930,  0.3077,  0.8380, -0.6366, -0.4464, -0.4341, -0.7936,\n",
            "        -0.2867, -0.0344,  1.3431,  0.3490])\n",
            "\n",
            "\n",
            "Token: 3 a\n",
            "tensor([-0.2709,  0.0440, -0.0203, -0.1740,  0.6444,  0.7121,  0.3551,  0.4714,\n",
            "        -0.2964,  0.5443, -0.7229, -0.0048,  0.0406,  0.0432,  0.2973,  0.1072,\n",
            "         0.4016, -0.5366,  0.0334,  0.0674,  0.6456, -0.0855,  0.1410,  0.0945,\n",
            "         0.7495, -0.1940, -0.6874, -0.4174, -0.2281,  0.1200, -0.4900,  0.8094,\n",
            "         0.0451, -0.1190,  0.2016,  0.3928, -0.2012,  0.3135,  0.7530,  0.2591,\n",
            "        -0.1157, -0.0293,  0.9350, -0.3607,  0.5242,  0.2371,  0.5271,  0.2287,\n",
            "        -0.5196, -0.7935, -0.2037, -0.5019,  0.1875,  0.9428, -0.4483, -3.6792,\n",
            "         0.0442, -0.2675,  2.1997,  0.2410, -0.0334,  0.6955, -0.6447, -0.0072,\n",
            "         0.8957,  0.2001,  0.4649,  0.6193, -0.1066,  0.0869, -0.4623,  0.1826,\n",
            "        -0.1585,  0.0208,  0.1937,  0.0634, -0.3167, -0.4818, -1.3848,  0.1367,\n",
            "         0.9686,  0.0500, -0.2738, -0.0357, -1.0577, -0.2447,  0.9037, -0.1244,\n",
            "         0.0808, -0.8340,  0.5720,  0.0889, -0.4253, -0.0183, -0.0800, -0.2858,\n",
            "        -0.0109, -0.4923,  0.6369,  0.2364])\n",
            "\n",
            "\n",
            "Token: 4 day\n",
            "tensor([-3.6689e-01,  4.1540e-01,  1.3478e-01, -1.7841e-01, -4.0050e-01,\n",
            "         2.0647e-01,  4.2742e-01,  1.1290e+00, -8.3981e-01, -4.7955e-01,\n",
            "         2.1115e-02, -1.8319e-01,  2.4588e-01, -2.9048e-02,  2.9683e-01,\n",
            "        -4.4613e-01,  1.7493e-01, -1.6619e-01, -4.9573e-01, -2.6547e-02,\n",
            "         7.6369e-01, -2.0810e-01,  2.6494e-01,  3.3085e-01,  5.7039e-01,\n",
            "         1.2832e-01, -2.8324e-01,  7.8652e-02,  3.8330e-01, -7.4012e-02,\n",
            "        -5.6725e-01, -2.5826e-01,  1.4013e-01,  1.1077e-01, -2.1629e-01,\n",
            "         4.0637e-01, -4.5740e-01,  2.7936e-01,  1.6717e-01, -1.6388e-01,\n",
            "        -6.9184e-01, -3.1493e-01,  6.5592e-01, -2.2426e-03, -1.6975e-02,\n",
            "         1.0777e-01, -1.8928e-01, -7.7079e-01,  2.4333e-01, -1.1467e+00,\n",
            "         1.2931e-01, -5.9668e-01,  5.2350e-01,  1.0476e+00, -8.3517e-01,\n",
            "        -2.6268e+00, -3.2837e-01,  2.4767e-01,  2.1535e+00,  6.0561e-01,\n",
            "        -5.4019e-01,  9.9737e-01, -1.8484e-01, -4.7044e-01, -1.9265e-02,\n",
            "         3.4195e-01,  3.1006e-01, -3.1499e-01,  2.9990e-01, -3.7647e-01,\n",
            "        -1.0041e-01, -4.8373e-01, -3.8982e-01, -3.4424e-01, -1.2195e-01,\n",
            "         5.8945e-01, -1.5675e-01, -3.3774e-01, -1.0446e+00, -3.7927e-02,\n",
            "         3.8237e-01, -6.3733e-02, -1.2990e-01,  1.2899e-01, -1.4625e+00,\n",
            "        -2.2875e-01, -3.6410e-01, -1.0337e-01,  2.3737e-01, -2.3498e-01,\n",
            "         1.5801e-01,  3.8339e-02,  1.3424e-02,  3.2535e-01, -1.1755e+00,\n",
            "         4.9695e-02,  5.7227e-01,  2.4380e-02,  2.2106e-01,  4.3169e-01])\n",
            "\n",
            "\n",
            "Token: 5 keeps\n",
            "tensor([-2.8212e-01,  8.1407e-02,  1.0041e+00, -2.1619e-01,  3.8559e-01,\n",
            "         5.8096e-01, -7.4637e-01,  9.6754e-02,  1.3090e-01, -5.7634e-01,\n",
            "         5.7288e-01, -1.5900e-01,  1.3582e-01,  2.7628e-01, -4.4655e-01,\n",
            "         1.5342e-01, -4.1672e-01, -2.2789e-01,  1.1763e+00,  4.3135e-04,\n",
            "        -2.0838e-01,  4.4750e-01, -1.4259e-01, -4.7203e-01,  2.8749e-01,\n",
            "         1.8887e-01, -8.8928e-01, -8.2278e-01, -3.1343e-01, -5.6226e-02,\n",
            "        -3.2358e-01,  4.7065e-01,  1.2294e-01,  1.3080e-01,  6.5288e-02,\n",
            "        -3.1797e-01, -2.3859e-01, -4.7632e-01, -2.3364e-02, -4.4278e-01,\n",
            "        -2.0717e-01, -4.5140e-01,  4.4082e-01, -3.7247e-01,  6.3666e-02,\n",
            "        -2.6637e-01, -3.7114e-01, -1.4971e-01,  2.7836e-01, -8.9750e-01,\n",
            "         7.6634e-01, -6.3621e-01, -7.2737e-02,  1.0848e+00,  2.2928e-01,\n",
            "        -1.6516e+00,  8.1383e-02,  5.1793e-02,  7.1174e-01,  2.2299e-01,\n",
            "         8.2580e-01,  3.6125e-02,  2.4621e-01, -1.3305e-01,  9.9692e-02,\n",
            "         7.9609e-01,  5.6096e-01,  1.4129e-01,  2.1741e-01,  2.1998e-01,\n",
            "         4.8408e-01, -9.3512e-02, -1.1885e-01, -4.0587e-02,  6.3986e-01,\n",
            "         2.3246e-01,  2.8093e-01, -2.3691e-01, -2.3029e-01,  2.4496e-01,\n",
            "         1.1252e-01,  2.6858e-02, -1.0912e+00, -8.9866e-02, -5.8186e-01,\n",
            "        -2.7347e-02, -2.6925e-01,  2.2656e-01, -6.4862e-03,  1.1973e-01,\n",
            "        -1.6618e-01,  1.5777e-01,  7.2022e-01, -7.2013e-01, -2.7190e-01,\n",
            "        -2.3454e-01,  1.6691e-01, -2.4730e-01,  3.4683e-01,  8.1010e-01])\n",
            "\n",
            "\n",
            "Token: 6 the\n",
            "tensor([-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344,\n",
            "        -0.5755,  0.0875,  0.2879, -0.0673,  0.3091, -0.2638, -0.1323, -0.2076,\n",
            "         0.3340, -0.3385, -0.3174, -0.4834,  0.1464, -0.3730,  0.3458,  0.0520,\n",
            "         0.4495, -0.4697,  0.0263, -0.5415, -0.1552, -0.1411, -0.0397,  0.2828,\n",
            "         0.1439,  0.2346, -0.3102,  0.0862,  0.2040,  0.5262,  0.1716, -0.0824,\n",
            "        -0.7179, -0.4153,  0.2033, -0.1276,  0.4137,  0.5519,  0.5791, -0.3348,\n",
            "        -0.3656, -0.5486, -0.0629,  0.2658,  0.3020,  0.9977, -0.8048, -3.0243,\n",
            "         0.0125, -0.3694,  2.2167,  0.7220, -0.2498,  0.9214,  0.0345,  0.4674,\n",
            "         1.1079, -0.1936, -0.0746,  0.2335, -0.0521, -0.2204,  0.0572, -0.1581,\n",
            "        -0.3080, -0.4162,  0.3797,  0.1501, -0.5321, -0.2055, -1.2526,  0.0716,\n",
            "         0.7056,  0.4974, -0.4206,  0.2615, -1.5380, -0.3022, -0.0734, -0.2831,\n",
            "         0.3710, -0.2522,  0.0162, -0.0171, -0.3898,  0.8742, -0.7257, -0.5106,\n",
            "        -0.5203, -0.1459,  0.8278,  0.2706])\n",
            "\n",
            "\n",
            "Token: 7 doctor\n",
            "tensor([ 0.0432, -0.4753,  0.1581,  0.2041, -0.1538,  0.7228,  0.2614,  0.2089,\n",
            "        -0.3147, -0.0703, -0.4337,  0.0531,  0.7363,  0.9811,  0.2353, -0.1045,\n",
            "         0.5026, -0.0334, -0.3554,  0.6455, -0.3710, -0.1005, -0.7693, -0.1696,\n",
            "        -0.1565,  0.5355,  0.3515, -1.5126,  0.0510,  0.2445, -0.3569,  0.4397,\n",
            "        -0.6298,  0.3289, -0.5301,  0.4983, -1.2061,  0.2780,  0.4273,  0.0958,\n",
            "        -0.4353,  0.9356,  0.3604, -0.8311,  0.1297, -0.1363, -0.5812,  0.0929,\n",
            "        -0.0147,  0.3256,  0.4120,  0.1451,  0.4980,  0.8693, -0.1803, -1.6227,\n",
            "        -0.6457,  0.1750,  0.7385,  0.3916,  0.8314,  0.5131,  0.1300, -0.2129,\n",
            "         0.6846,  0.0563,  0.0908,  0.2803, -0.1223,  0.6076, -0.5791, -0.0241,\n",
            "        -0.0633,  0.4075,  0.1077,  0.5798,  0.0928, -0.1559, -0.3649, -0.4663,\n",
            "         0.3755,  0.1640, -0.7514, -0.1083, -2.1363, -0.1166,  0.2316,  0.2442,\n",
            "        -0.3635, -0.3493,  0.6000, -0.1053,  0.7861,  0.4571, -0.1712,  0.6362,\n",
            "         0.6862, -0.2840, -0.2977, -0.8115])\n",
            "\n",
            "\n",
            "Token: 8 away\n",
            "tensor([-0.1038, -0.0148,  0.5993, -0.5132, -0.0365,  0.6588, -0.5791,  0.1782,\n",
            "         0.2366, -0.2138,  0.5534,  0.5360,  0.0414,  0.1610,  0.0171, -0.3724,\n",
            "         0.0180,  0.3927, -0.2326,  0.1818,  0.6640,  0.9816,  0.4234,  0.0306,\n",
            "         0.3501,  0.2552, -0.7118, -0.4218,  0.1307, -0.4745, -0.0817,  0.1574,\n",
            "        -0.1326,  0.2268, -0.1689, -0.1112, -0.3227, -0.0210, -0.4335,  0.1720,\n",
            "        -0.6737, -0.7905,  0.1056, -0.4219, -0.1239, -0.0635, -0.1784,  0.5636,\n",
            "         0.1699, -0.1780,  0.1396, -0.2017,  0.0790,  1.4497,  0.2356, -2.6014,\n",
            "        -0.5286, -0.1164,  1.7184,  0.3325,  0.1214,  1.1602, -0.2914,  0.4712,\n",
            "         0.4187,  0.3527,  0.4787, -0.0423, -0.1829,  0.1796, -0.2443, -0.3404,\n",
            "         0.2034, -0.9368,  0.0131,  0.0803, -0.3660, -0.4401, -0.3539,  0.1591,\n",
            "         0.5581,  0.1492, -0.8643,  0.0403, -1.0939, -0.2639, -0.2949,  0.2570,\n",
            "        -0.3372, -0.0865, -0.2425, -0.2111,  0.0996,  0.1282, -0.7871, -0.5178,\n",
            "        -0.1094,  0.9763,  0.5703,  0.1358])\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for token in sentence_2:\n",
        "    print(token)\n",
        "    print(token.embedding)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0mUTmTUG8d9",
        "outputId": "a9078564-78b9-4ffd-f7d5-cc512ef86a99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Token: 2 apple"
            ]
          },
          "execution_count": 14,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_2[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAiqdm4WHAAt",
        "outputId": "c31a4ff8-b0df-4350-dfa8-2905fad5d14d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.5985, -0.4632,  0.1300, -0.0196,  0.4603, -0.3018,  0.8977, -0.6563,\n",
              "         0.6686, -0.4916,  0.0376, -0.0509,  0.6451, -0.5388, -0.3765, -0.0431,\n",
              "         0.5138,  0.1778,  0.2860,  0.9206, -0.4935, -0.4858,  0.6132,  0.7821,\n",
              "         0.1925,  0.9123, -0.0556, -0.1251, -0.6569,  0.0686,  0.5563,  1.6110,\n",
              "        -0.0074, -0.4888,  0.4549,  0.9610, -0.0634,  0.1743,  0.9814, -1.3125,\n",
              "        -0.1580, -0.5430, -0.1389, -0.2615, -0.3691,  0.2684, -0.2438, -0.1948,\n",
              "         0.6258, -0.7377,  0.3835, -0.7500, -0.3905,  0.0915, -0.3659, -1.4715,\n",
              "        -0.4523,  0.2256,  1.1412, -0.3853, -0.0672,  0.5729, -0.3919,  0.3130,\n",
              "        -0.2923, -0.9616,  0.1515, -0.2166,  0.2510,  0.0970,  0.2843,  1.4296,\n",
              "        -0.5056, -0.5137, -0.4722,  0.3204,  0.0231,  0.2262, -0.0972,  0.8213,\n",
              "         0.9260, -1.0086, -0.3864,  0.8641, -1.2060, -0.2853,  0.2265, -0.3877,\n",
              "         0.4088,  0.5930,  0.3077,  0.8380, -0.6366, -0.4464, -0.4341, -0.7936,\n",
              "        -0.2867, -0.0344,  1.3431,  0.3490])"
            ]
          },
          "execution_count": 15,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_2[1].embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTGb9xJAHBVv",
        "outputId": "61b35649-c93b-4a7e-f486-55cb2f607869"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([100])"
            ]
          },
          "execution_count": 16,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_2[1].embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtoyNXaLHEiW"
      },
      "source": [
        "Glove Distance between the same word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCgdG373HCoZ"
      },
      "outputs": [],
      "source": [
        "glove_dst = distance.euclidean(np.array(sentence_1[0].embedding),\n",
        "                               np.array(sentence_2[1].embedding))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk4jpOXSHGad",
        "outputId": "beea0b63-34dc-446a-99f2-216b679a01fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distance between apple embeddings for Glove = 0.0\n"
          ]
        }
      ],
      "source": [
        "print(\"Distance between apple embeddings for Glove = {}\".format(glove_dst))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K1ApoUzHJlO"
      },
      "source": [
        "Bert Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163,
          "referenced_widgets": [
            "2fec018a8c914a099090a1fd33a857e8",
            "88502a3f6f05453a8df6930c82d56ed0",
            "b0fb4cbb7e764cb89868889adef8906b",
            "ede1437be3714135916989b756037f74",
            "9b0bdb8d58284c13b97b29e1f961fef8",
            "ad43b0269b3e448a89fdae9d37042aed",
            "34ccc1bfe764485f848be7162f846217",
            "e49538c37df640738b5281b6aaa8a6b3",
            "1875c336e93b4923a013a345606343d3",
            "38509d93cd044da5970ea0e3e8dedd5d",
            "279b590d3b794b2eb089e78e51b94a22",
            "e35ff9b168be45f285d5e6d6b0858024",
            "84b063e752734300bd7a683e82fc1567",
            "15cb804d51834b9db7a9134268a80e5c",
            "d43e5c48b1ad4ef78c9e574d1e3f8670",
            "4a27f9cef77f419499af21dc346c4a5e",
            "31d079fc9b3b46b4a05fb92ed3028030",
            "5ff1a9f3e08f428e8b693ac80a0025dd",
            "5106c02be5814b3796dd3f9f22cba957",
            "29209ef7e663423ea88b0f5c4c283bb3",
            "17784b2f99d94faa86246a040b72296a",
            "49e82674d6aa43e881ebff1c425ce041",
            "0633eef4dc234f5db838a79bcf53f9c0",
            "86ee11de49f54bf0bd3654a9fa460d60"
          ]
        },
        "id": "NjC2cJ8sHHqP",
        "outputId": "63f74da9-4bce-4a32-b35b-4a204808c057"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fec018a8c914a099090a1fd33a857e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1875c336e93b4923a013a345606343d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31d079fc9b3b46b4a05fb92ed3028030",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=714314041.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "bert_embedding = TransformerWordEmbeddings('bert-base-multilingual-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX5-e6hYHLZc",
        "outputId": "d1858e7a-0f9a-4ac4-dc11-c0973a4550f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token: 1 apple\n",
            "tensor([-0.5985, -0.4632,  0.1300,  ...,  1.4355,  0.0781,  0.2832])\n",
            "Token: 2 released\n",
            "tensor([-0.0451, -0.7260,  0.6271,  ...,  0.8646,  0.4700,  0.4862])\n",
            "Token: 3 iphone\n",
            "tensor([-0.3609, -0.5975,  0.5235,  ...,  0.8601,  0.1087, -0.7263])\n",
            "Token: 4 12\n",
            "tensor([ 0.0730,  0.1813,  0.0204,  ...,  1.0590,  0.4350, -0.1114])\n",
            "Token: 5 pro\n",
            "tensor([-0.2332,  0.1988, -0.2101,  ...,  1.8614,  0.7496, -0.2961])\n",
            "Token: 6 max\n",
            "tensor([ 0.3356,  0.1247,  0.2099,  ...,  1.3470, -0.0816,  0.4086])\n",
            "Token: 7 in\n",
            "tensor([ 0.0857, -0.2220,  0.1657,  ...,  1.4064, -0.2576, -0.0673])\n",
            "Token: 8 2020\n",
            "tensor([-0.1241,  0.3447,  0.9852,  ...,  0.8098,  1.0564, -0.2190])\n"
          ]
        }
      ],
      "source": [
        "bert_embedding.embed(sentence_1)\n",
        "for token in sentence_1:\n",
        "    print(token)\n",
        "    print(token.embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULbIqa9WHN5N",
        "outputId": "ed5015ff-77f7-46dd-f1d2-62edd68e98b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Token: 1 apple"
            ]
          },
          "execution_count": 21,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_1[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdCeeb_ZHPYn",
        "outputId": "0a3a6e79-bf91-4da7-ebc8-49c76bf95312"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.5985, -0.4632,  0.1300,  ...,  1.4355,  0.0781,  0.2832])"
            ]
          },
          "execution_count": 22,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_1[0].embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTlSGGngHQmF",
        "outputId": "43b6ca53-8025-4b5c-c21d-11bbbca5bd47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3172])"
            ]
          },
          "execution_count": 23,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_1[0].embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2QvuYTxHR-v",
        "outputId": "f0d4f2ac-5da9-412a-8324-8767fc9c7cb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token: 1 an\n",
            "tensor([-0.4214, -0.1880,  0.4624,  ...,  1.2605,  0.8692,  0.1740])\n",
            "Token: 2 apple\n",
            "tensor([-0.5985, -0.4632,  0.1300,  ...,  1.8314,  1.0177,  0.2707])\n",
            "Token: 3 a\n",
            "tensor([-0.2709,  0.0440, -0.0203,  ...,  0.8354, -0.3497,  0.5959])\n",
            "Token: 4 day\n",
            "tensor([-0.3669,  0.4154,  0.1348,  ...,  0.9259, -0.0530,  0.1594])\n",
            "Token: 5 keeps\n",
            "tensor([-0.2821,  0.0814,  1.0041,  ...,  0.1253,  0.6754, -0.1647])\n",
            "Token: 6 the\n",
            "tensor([-0.0382, -0.2449,  0.7281,  ..., -0.1876, -0.3076,  0.5679])\n",
            "Token: 7 doctor\n",
            "tensor([ 0.0432, -0.4753,  0.1581,  ...,  0.7838,  0.3551,  0.3403])\n",
            "Token: 8 away\n",
            "tensor([-0.1038, -0.0148,  0.5993,  ...,  1.2423,  0.4363,  0.1388])\n"
          ]
        }
      ],
      "source": [
        "bert_embedding.embed(sentence_2)\n",
        "\n",
        "for token in sentence_2:\n",
        "    print(token)\n",
        "    print(token.embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnwHXuGGHUEb",
        "outputId": "05c3c9b3-a9b0-4a81-c160-81b97427e7b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Token: 2 apple"
            ]
          },
          "execution_count": 25,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_2[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME1ZrqubHV-I",
        "outputId": "89b0a4ab-b9b2-4bac-d50f-66d13ac34b8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.5985, -0.4632,  0.1300,  ...,  1.8314,  1.0177,  0.2707])"
            ]
          },
          "execution_count": 26,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_2[1].embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nasUhC54HXWr",
        "outputId": "926934fc-b82a-4a61-d7b3-a1da6fcfa3e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sent2vec\n",
            "  Downloading sent2vec-0.2.0-py3-none-any.whl (5.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sent2vec) (1.19.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from sent2vec) (3.6.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from sent2vec) (2.2.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from sent2vec) (1.9.0+cu102)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->sent2vec) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->sent2vec) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->sent2vec) (5.1.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (57.2.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->sent2vec) (4.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->sent2vec) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->sent2vec) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->sent2vec) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->sent2vec) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->sent2vec) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->sent2vec) (1.24.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->sent2vec) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 39.1 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 44.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers->sent2vec) (21.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->sent2vec) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 49.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers->sent2vec) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->sent2vec) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->sent2vec) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers, sent2vec\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 sent2vec-0.2.0 tokenizers-0.10.3 transformers-4.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sent2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPgts-prZbXX",
        "outputId": "363cdecd-0400-4069-9efd-085f498abbdf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dist_1: 0.02920389175415039, dist_2: 0.10352593660354614\n"
          ]
        }
      ],
      "source": [
        "#### another method for BERT\n",
        "from scipy import spatial\n",
        "from sent2vec.vectorizer import Vectorizer\n",
        "\n",
        "sentences = [\n",
        "    \"latte al cioccolato\",\n",
        "    \"Milka  cioccolato al latte 100 g\",\n",
        "    \"Danone, HiPRO 25g Proteine gusto cioccolato 330 ml\",\n",
        "]\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "vectorizer.bert(sentences)\n",
        "vectors_bert = vectorizer.vectors\n",
        "\n",
        "dist_1 = spatial.distance.cosine(vectors_bert[0], vectors_bert[1])\n",
        "dist_2 = spatial.distance.cosine(vectors_bert[0], vectors_bert[2])\n",
        "print('dist_1: {0}, dist_2: {1}'.format(dist_1, dist_2))\n",
        "# dist_1: 0.043, dist_2: 0.192"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O66VyInjZgDZ"
      },
      "source": [
        "# ELMO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxHvLe-AZkTA",
        "outputId": "4c94f84e-958b-4d53-c278-57e8bf576afa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow>=1.7.0 in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (0.4.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (0.36.2)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (1.1.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (0.2.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (3.7.4.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (1.6.3)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (1.12.1)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (1.34.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (2.5.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (1.12)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (3.12.4)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (1.15.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (2.5.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=1.7.0) (1.5.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow>=1.7.0) (57.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.7.0) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.7.0) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.7.0) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.7.0) (0.4.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.7.0) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.7.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.7.0) (1.30.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=1.7.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=1.7.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=1.7.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=1.7.0) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=1.7.0) (4.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=1.7.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=1.7.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=1.7.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=1.7.0) (4.2.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=1.7.0) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=1.7.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=1.7.0) (0.4.8)\n"
          ]
        }
      ],
      "source": [
        "# !pip install \"tensorflow>=1.7.0\"\n",
        "#!pip install tensorflow-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RW0-hk0BZwNG"
      },
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "\n",
        "elmo = hub.load(\"https://tfhub.dev/google/elmo/2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "AbUHWCWPZ1dT",
        "outputId": "c0f93b22-a510-47ac-cf4e-f3d50b421b5a"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-de27cc7bfb53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# x = [\"Nothing suits me like suit\"] # Extract ELMo features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# embeddings = elmo(x, signature=\"default\", as_dict=True)[\"elmo\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0melmo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'default'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"testing\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0melmo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1709\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mdo\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m     \"\"\"\n\u001b[0;32m-> 1711\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/wrap_function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m       return super(WrappedFunction, self)._call_impl(\n\u001b[0;32m--> 247\u001b[0;31m           args, kwargs, cancellation_manager)\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_signature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1727\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mstructured_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1729\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_with_flat_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_with_flat_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_with_flat_signature\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1775\u001b[0m         raise TypeError(\"{}: expected argument #{}(zero-based) to be a Tensor; \"\n\u001b[1;32m   1776\u001b[0m                         \"got {} ({})\".format(self._flat_signature_summary(), i,\n\u001b[0;32m-> 1777\u001b[0;31m                                              type(arg).__name__, str(arg)))\n\u001b[0m\u001b[1;32m   1778\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: pruned(text): expected argument #0(zero-based) to be a Tensor; got list (['testing', 'batch'])"
          ]
        }
      ],
      "source": [
        "# x = [\"Nothing suits me like suit\"] # Extract ELMo features\n",
        "# embeddings = elmo(x, signature=\"default\", as_dict=True)[\"elmo\"]\n",
        "elmo.signatures['default']([\"testing\", 'batch'])\n",
        "elmo.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSweRA1oasQk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP4uVBuQMRtH"
      },
      "source": [
        "# GPT-3 Clone GPT-Neo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz0npMZGMWXT",
        "outputId": "394fcb05-faf0-4c50-f34a-24211d48b03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.1+cu111\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (1982.2MB)\n",
            "\u001b[K     |█████████████▌                  | 834.1MB 1.9MB/s eta 0:09:50tcmalloc: large alloc 1147494400 bytes == 0x55a491942000 @  0x7f41a3a70615 0x55a4580f0cdc 0x55a4581d052a 0x55a4580f3afd 0x55a4581e4fed 0x55a458167988 0x55a4581624ae 0x55a4580f53ea 0x55a4581677f0 0x55a4581624ae 0x55a4580f53ea 0x55a45816432a 0x55a4581e5e36 0x55a458163853 0x55a4581e5e36 0x55a458163853 0x55a4581e5e36 0x55a458163853 0x55a4581e5e36 0x55a4582683e1 0x55a4581c86a9 0x55a458133cc4 0x55a4580f4559 0x55a4581684f8 0x55a4580f530a 0x55a4581633b5 0x55a4581627ad 0x55a4580f53ea 0x55a4581633b5 0x55a4580f530a 0x55a4581633b5\n",
            "\u001b[K     |█████████████████               | 1055.7MB 1.5MB/s eta 0:10:12tcmalloc: large alloc 1434370048 bytes == 0x55a4d5f98000 @  0x7f41a3a70615 0x55a4580f0cdc 0x55a4581d052a 0x55a4580f3afd 0x55a4581e4fed 0x55a458167988 0x55a4581624ae 0x55a4580f53ea 0x55a4581677f0 0x55a4581624ae 0x55a4580f53ea 0x55a45816432a 0x55a4581e5e36 0x55a458163853 0x55a4581e5e36 0x55a458163853 0x55a4581e5e36 0x55a458163853 0x55a4581e5e36 0x55a4582683e1 0x55a4581c86a9 0x55a458133cc4 0x55a4580f4559 0x55a4581684f8 0x55a4580f530a 0x55a4581633b5 0x55a4581627ad 0x55a4580f53ea 0x55a4581633b5 0x55a4580f530a 0x55a4581633b5\n",
            "\u001b[K     |█████████████████████▋          | 1336.2MB 76.1MB/s eta 0:00:09tcmalloc: large alloc 1792966656 bytes == 0x55a45adca000 @  0x7f41a3a70615 0x55a4580f0cdc 0x55a4581d052a 0x55a4580f3afd 0x55a4581e4fed 0x55a458167988 0x55a4581624ae 0x55a4580f53ea 0x55a4581677f0 0x55a4581624ae 0x55a4580f53ea 0x55a45816432a 0x55a4581e5e36 0x55a458163853 0x55a4581e5e36 0x55a458163853 0x55a4581e5e36 0x55a458163853 0x55a4581e5e36 0x55a4582683e1 0x55a4581c86a9 0x55a458133cc4 0x55a4580f4559 0x55a4581684f8 0x55a4580f530a 0x55a4581633b5 0x55a4581627ad 0x55a4580f53ea 0x55a4581633b5 0x55a4580f530a 0x55a4581633b5\n",
            "\u001b[K     |███████████████████████████▎    | 1691.1MB 1.3MB/s eta 0:03:43tcmalloc: large alloc 2241208320 bytes == 0x55a4c5bb2000 @  0x7f41a3a70615 0x55a4580f0cdc 0x55a4581d052a 0x55a4580f3afd 0x55a4581e4fed 0x55a458167988 0x55a4581624ae 0x55a4580f53ea 0x55a4581677f0 0x55a4581624ae 0x55a4580f53ea 0x55a45816432a 0x55a4581e5e36 0x55a458163853 0x55a4581e5e36 0x55a458163853 0x55a4581e5e36 0x55a458163853 0x55a4581e5e36 0x55a4582683e1 0x55a4581c86a9 0x55a458133cc4 0x55a4580f4559 0x55a4581684f8 0x55a4580f530a 0x55a4581633b5 0x55a4581627ad 0x55a4580f53ea 0x55a4581633b5 0x55a4580f530a 0x55a4581633b5\n",
            "\u001b[K     |████████████████████████████████| 1982.2MB 1.3MB/s eta 0:00:01tcmalloc: large alloc 1982177280 bytes == 0x55a54b514000 @  0x7f41a3a6f1e7 0x55a458126f37 0x55a4580f0cdc 0x55a4581d052a 0x55a4580f3afd 0x55a4581e4fed 0x55a458167988 0x55a4581624ae 0x55a4580f53ea 0x55a45816360e 0x55a4581624ae 0x55a4580f53ea 0x55a45816360e 0x55a4581624ae 0x55a4580f53ea 0x55a45816360e 0x55a4581624ae 0x55a4580f53ea 0x55a45816360e 0x55a4581624ae 0x55a4580f53ea 0x55a45816360e 0x55a4580f530a 0x55a45816360e 0x55a4581624ae 0x55a4580f53ea 0x55a45816432a 0x55a4581624ae 0x55a4580f53ea 0x55a45816432a 0x55a4581624ae\n",
            "tcmalloc: large alloc 2477727744 bytes == 0x55a635c2c000 @  0x7f41a3a70615 0x55a4580f0cdc 0x55a4581d052a 0x55a4580f3afd 0x55a4581e4fed 0x55a458167988 0x55a4581624ae 0x55a4580f53ea 0x55a45816360e 0x55a4581624ae 0x55a4580f53ea 0x55a45816360e 0x55a4581624ae 0x55a4580f53ea 0x55a45816360e 0x55a4581624ae 0x55a4580f53ea 0x55a45816360e 0x55a4581624ae 0x55a4580f53ea 0x55a45816360e 0x55a4580f530a 0x55a45816360e 0x55a4581624ae 0x55a4580f53ea 0x55a45816432a 0x55a4581624ae 0x55a4580f53ea 0x55a45816432a 0x55a4581624ae 0x55a4580f5a81\n",
            "\u001b[K     |████████████████████████████████| 1982.2MB 3.7kB/s \n",
            "\u001b[?25hCollecting torchvision==0.9.1+cu111\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.9.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (17.6MB)\n",
            "\u001b[K     |████████████████████████████████| 17.6MB 130kB/s \n",
            "\u001b[?25hCollecting torchaudio===0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/55/01ad9244bcd595e39cea5ce30726a7fe02fd963d07daeb136bfe7e23f0a5/torchaudio-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cu111) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cu111) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.1+cu111) (7.1.2)\n",
            "\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.8.1+cu111 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "Successfully installed torch-1.8.1+cu111 torchaudio-0.8.1 torchvision-0.9.1+cu111\n"
          ]
        }
      ],
      "source": [
        "# 1. Install and Import Dependencies¶\n",
        "!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLkEEDFoMob-",
        "outputId": "133f6473-834c-44c2-c4b7-fd070d788b00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/92/6153f4912b84ee1ab53ab45663d23e7cf3704161cb5ef18b0c07e207cef2/transformers-4.7.0-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 51.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Installing collected packages: huggingface-hub, sacremoses, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S8WDTDW4Msu0"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline # First line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT52_DKDMue1"
      },
      "outputs": [],
      "source": [
        "# 2. Setup Generator\n",
        "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B') # Second line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dbn6zqK5MwL7"
      },
      "outputs": [],
      "source": [
        "# 3. Generate Text using Prompt\n",
        "prompt = \"The current stock market\" # Third line\n",
        "res = generator(prompt, max_length=50, do_sample=True, temperature=0.9) # Fourth line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eFlFb1sM53w"
      },
      "outputs": [],
      "source": [
        "# 4. Output Text\n",
        "print(res[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vid3tLw3M_Uf",
        "outputId": "73e61a21-1f55-4669-a46a-1f312ea1b518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The cat\n",
            "the dog sleep\n",
            "the basket\n",
            "the door\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'The cat and the dog sleep in the basket near the door.')\n",
        "for np in doc.noun_chunks:\n",
        "  print(np.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOsSRtJCoCuO",
        "outputId": "166d987b-06a9-486c-b7dd-58948691014b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<generator object at 0x7f2907bb7f50>\n"
          ]
        }
      ],
      "source": [
        "print(doc.noun_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdu-7KPpoNK2",
        "outputId": "8d078d7c-2542-4545-a768-6ecf918695d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'The cat and the dog sleep in the basket near the door.')\n",
        "print(len([token.pos_ for token in doc if token.pos_==\"NOUN\"])>0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PnbAMtrrNwE",
        "outputId": "64671c4e-b48b-4238-e325-021895a7b334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POS_: ['DET', 'NOUN', 'CCONJ', 'DET', 'NOUN', 'NOUN', 'ADP', 'DET', 'NOUN', 'SCONJ', 'DET', 'NOUN', 'PUNCT']\n"
          ]
        }
      ],
      "source": [
        "print(\"POS_:\",[token.pos_ for token in doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTHxatcUrQYB",
        "outputId": "d01da1fd-8fac-4bea-ab02-9f8e48215cc8"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-cb30d1ccfe10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Today's date:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
          ]
        }
      ],
      "source": [
        "from datetime import date\n",
        "\n",
        "today = date.today()\n",
        "print(\"Today's date:\", today)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jqb2qGgXzVX8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All Word Embedding into one"
      ],
      "metadata": {
        "id": "zX_HN8fhX3rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CountVectorizer\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load your dataset as a pandas DataFrame\n",
        "data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Split the dataset into independent and dependent variables\n",
        "X = data[['numeric_column', 'text_column', 'categorical_column']]  # Independent variables\n",
        "y = data['target_column']  # Dependent variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the preprocessing steps for numeric, text, and categorical columns\n",
        "numeric_features = ['numeric_column']\n",
        "numeric_transformer = StandardScaler()\n",
        "\n",
        "text_features = ['text_column']\n",
        "text_transformer = CountVectorizer()\n",
        "\n",
        "categorical_features = ['categorical_column']\n",
        "categorical_transformer = OneHotEncoder()\n",
        "\n",
        "# Create the column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('text', text_transformer, text_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create the pipeline with preprocessing and modeling steps\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict the target variable for the test data\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "lNH6SB-nX8AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TfidfVectorizer\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load your dataset as a pandas DataFrame\n",
        "data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Split the dataset into independent and dependent variables\n",
        "X = data[['numeric_column', 'text_column', 'categorical_column']]  # Independent variables\n",
        "y = data['target_column']  # Dependent variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the preprocessing steps for numeric, text, and categorical columns\n",
        "numeric_features = ['numeric_column']\n",
        "numeric_transformer = StandardScaler()\n",
        "\n",
        "text_features = ['text_column']\n",
        "text_transformer = TfidfVectorizer()\n",
        "\n",
        "categorical_features = ['categorical_column']\n",
        "categorical_transformer = OneHotEncoder()\n",
        "\n",
        "# Create the column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('text', text_transformer, text_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create the pipeline with preprocessing and modeling steps\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict the target variable for the test data\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "tLDj68OBYVPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Word2Vec\n",
        "  #If you want to use Word2Vec instead of TF-IDF vectorization for the text column, you can use the gensim library to train a Word2Vec model. Here's an example of how you can modify the code:\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "# Load your dataset as a pandas DataFrame\n",
        "data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Split the dataset into independent and dependent variables\n",
        "X = data[['numeric_column', 'text_column', 'categorical_column']]  # Independent variables\n",
        "y = data['target_column']  # Dependent variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the preprocessing steps for numeric, text, and categorical columns\n",
        "numeric_features = ['numeric_column']\n",
        "numeric_transformer = StandardScaler()\n",
        "\n",
        "categorical_features = ['categorical_column']\n",
        "categorical_transformer = OneHotEncoder()\n",
        "\n",
        "# Train the Word2Vec model on the text data\n",
        "text_data = X_train['text_column'].values.tolist()\n",
        "text_data = [text.split() for text in text_data]\n",
        "word2vec_model = Word2Vec(text_data, size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Function to convert text data to Word2Vec embeddings\n",
        "def word2vec_transform(text_data):\n",
        "    vectors = []\n",
        "    for text in text_data:\n",
        "        text_vector = np.zeros(100)\n",
        "        count = 0\n",
        "        for word in text.split():\n",
        "            if word in word2vec_model.wv.vocab:\n",
        "                text_vector += word2vec_model.wv[word]\n",
        "                count += 1\n",
        "        if count != 0:\n",
        "            text_vector /= count\n",
        "        vectors.append(text_vector)\n",
        "    return np.array(vectors)\n",
        "\n",
        "# Create the column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('text', FunctionTransformer(word2vec_transform), 'text_column'),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create the pipeline with preprocessing and modeling steps\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict the target variable for the test data\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "2cGHzq5ZZEmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "# Load your dataset as a pandas DataFrame\n",
        "data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Split the dataset into independent and dependent variables\n",
        "X = data[['numeric_column', 'text_column', 'categorical_column']]  # Independent variables\n",
        "y = data['target_column']  # Dependent variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the preprocessing steps for numeric, text, and categorical columns\n",
        "numeric_features = ['numeric_column']\n",
        "numeric_transformer = StandardScaler()\n",
        "\n",
        "categorical_features = ['categorical_column']\n",
        "categorical_transformer = OneHotEncoder()\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Custom transformer for BERT text encoding\n",
        "class BertTextEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        input_ids = tokenizer.batch_encode_plus(\n",
        "            X,\n",
        "            add_special_tokens=True,\n",
        "            padding='longest',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )['input_ids'].to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)[0][:, 0, :].cpu().numpy()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# Create the column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('text', BertTextEncoder(), 'text_column'),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create the pipeline with preprocessing and modeling steps\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict the target variable for the test data\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "3w4mlp8qZMkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ELMO\n",
        "  ###To use ELMo embeddings for the text column instead of TfidfVectorizer, you'll need to utilize a pre-trained ELMo model and the tensorflow library. Here's an example of how you can modify the code:\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load your dataset as a pandas DataFrame\n",
        "data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Split the dataset into independent and dependent variables\n",
        "X = data[['numeric_column', 'text_column', 'categorical_column']]  # Independent variables\n",
        "y = data['target_column']  # Dependent variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the pre-trained ELMo model\n",
        "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
        "\n",
        "# Define the preprocessing steps for numeric, text, and categorical columns\n",
        "numeric_features = ['numeric_column']\n",
        "numeric_transformer = StandardScaler()\n",
        "\n",
        "categorical_features = ['categorical_column']\n",
        "categorical_transformer = OneHotEncoder()\n",
        "\n",
        "# Define a custom transformer for the text column using ELMo\n",
        "class ElmoTransformer:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def transform(self, X):\n",
        "        embeddings = tf.squeeze(self.model.signatures[\"default\"](tf.constant(X)))\n",
        "        return embeddings.numpy()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        pass\n",
        "\n",
        "# Create the column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('text', ElmoTransformer(elmo), 'text_column'),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create the pipeline with preprocessing and modeling steps\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict the target variable for the test data\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "id": "pId7KDBAZ5HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FastText\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import fasttext\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load your dataset as a pandas DataFrame\n",
        "data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Split the dataset into independent and dependent variables\n",
        "X = data[['numeric_column', 'text_column', 'categorical_column']]  # Independent variables\n",
        "y = data['target_column']  # Dependent variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the preprocessing steps for numeric, text, and categorical columns\n",
        "numeric_features = ['numeric_column']\n",
        "numeric_transformer = StandardScaler()\n",
        "\n",
        "text_features = ['text_column']\n",
        "\n",
        "categorical_features = ['categorical_column']\n",
        "categorical_transformer = OneHotEncoder()\n",
        "\n",
        "# Create the column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features),\n",
        "        ('text', 'passthrough', text_features)\n",
        "    ])\n",
        "\n",
        "# Create the pipeline with preprocessing and modeling steps\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Preprocess the text column using FastText and obtain text embeddings\n",
        "X_train_text = pipeline.named_steps['preprocessor'].transformers_[2][1].transform(X_train[text_features])\n",
        "X_test_text = pipeline.named_steps['preprocessor'].transformers_[2][1].transform(X_test[text_features])\n",
        "\n",
        "# Train FastText model on the training text data\n",
        "model = fasttext.train_unsupervised(X_train_text, model='skipgram')\n",
        "\n",
        "# Convert text data to embeddings\n",
        "X_train_text_emb = [model.get_sentence_vector(text) for text in X_train_text]\n",
        "X_test_text_emb = [model.get_sentence_vector(text) for text in X_test_text]\n",
        "\n",
        "# Combine the preprocessed numeric, categorical, and text embeddings\n",
        "X_train_combined = pd.concat([pd.DataFrame(X_train[numeric_features].values),\n",
        "                              pd.DataFrame(X_train_text_emb),\n",
        "                              pd.DataFrame(X_train[categorical_features].values)], axis=1)\n",
        "\n",
        "X_test_combined = pd.concat([pd.DataFrame(X_test[numeric_features].values),\n",
        "                             pd.DataFrame(X_test_text_emb),\n",
        "                             pd.DataFrame(X_test[categorical_features].values)], axis=1)\n",
        "\n",
        "# Fit the classifier on the combined data\n",
        "pipeline.named_steps['classifier'].fit(X_train_combined, y_train)\n",
        "\n",
        "# Predict the target variable for the test data\n",
        "y_pred = pipeline.named_steps['classifier'].predict(X_test_combined)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "id": "xaWPdFQhaJyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GPT-3\n",
        "import pandas as pd\n",
        "import openai\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Set up your OpenAI API credentials\n",
        "openai.api_key = 'YOUR_API_KEY'\n",
        "\n",
        "# Load your dataset as a pandas DataFrame\n",
        "data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Split the dataset into independent and dependent variables\n",
        "X = data[['numeric_column', 'text_column', 'categorical_column']]  # Independent variables\n",
        "y = data['target_column']  # Dependent variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the preprocessing steps for numeric, text, and categorical columns\n",
        "numeric_features = ['numeric_column']\n",
        "numeric_transformer = StandardScaler()\n",
        "\n",
        "categorical_features = ['categorical_column']\n",
        "categorical_transformer = OneHotEncoder()\n",
        "\n",
        "# Create the GPT-3 text encoder function\n",
        "def gpt3_encoder(text):\n",
        "    response = openai.Completion.create(\n",
        "        engine='davinci-codex',\n",
        "        prompt=text,\n",
        "        max_tokens=100,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# Create the column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('text', FunctionTransformer(gpt3_encoder), 'text_column'),  # Use gpt3_encoder function\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create the pipeline with preprocessing and modeling steps\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict the target variable for the test data\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "Y2nHP2MJagNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Names Entity Recognition Custom-(NER)\n",
        "Building Custom Named Entity Recognition Model Using Spacy\n",
        "\n"
      ],
      "metadata": {
        "id": "SH76MOw9OYad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08-QqjN5OqR2",
        "outputId": "30d07cc1-1bf9-4043-a3ad-28d826c6b210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-28 17:40:38.803556: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-lg==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.1.3)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlSWShC_OwGb",
        "outputId": "7f43207c-11c9-468a-f057-ca0c0061b8ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x78b20e683040>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Donad Trump was President of USA\")\n",
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXZLy5UiOxw6",
        "outputId": "d32a9ef8-d70d-4156-f153-46f08a05d898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Donad Trump was President of USA"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KfXO7hhO0dJ",
        "outputId": "a96bf6b1-5900-4e54-ddfe-4c0316338b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc.ents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDg5evboO2W4",
        "outputId": "d64d636d-8268-4ca4-d44c-3d7cdbdf5c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Donad Trump, USA)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc.ents[0], type(doc.ents[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjiHplVAO345",
        "outputId": "68127003-97c3-4cb8-fa3e-1faa1ebae54f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Donad Trump, spacy.tokens.span.Span)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "RangMkU7O54l",
        "outputId": "93d3ad52-70a7-4b74-d1c0-19e87bc7916a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Donad Trump\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " was President of \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    USA\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# https://www.kaggle.com/datasets/finalepoch/medical-ner\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/Datasets/Corona2.json', 'r') as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "eQEmCb0pO9aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['examples'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W71n5sDoO_ih",
        "outputId": "8b245fc6-975e-4531-f749-c0a1fcdf821d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              " 'content': \"While bismuth compounds (Pepto-Bismol) decreased the number of bowel movements in those with travelers' diarrhea, they do not decrease the length of illness.[91] Anti-motility agents like loperamide are also effective at reducing the number of stools but not the duration of disease.[8] These agents should be used only if bloody diarrhea is not present.[92]\\n\\nDiosmectite, a natural aluminomagnesium silicate clay, is effective in alleviating symptoms of acute diarrhea in children,[93] and also has some effects in chronic functional diarrhea, radiation-induced diarrhea, and chemotherapy-induced diarrhea.[45] Another absorbent agent used for the treatment of mild diarrhea is kaopectate.\\n\\nRacecadotril an antisecretory medication may be used to treat diarrhea in children and adults.[86] It has better tolerability than loperamide, as it causes less constipation and flatulence.[94]\",\n",
              " 'metadata': {},\n",
              " 'annotations': [{'id': '0825a1bf-6a6e-4fa2-be77-8d104701eaed',\n",
              "   'tag_id': 'c06bd022-6ded-44a5-8d90-f17685bb85a1',\n",
              "   'end': 371,\n",
              "   'start': 360,\n",
              "   'example_id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              "   'tag_name': 'Medicine',\n",
              "   'value': 'Diosmectite',\n",
              "   'correct': None,\n",
              "   'human_annotations': [{'timestamp': '2020-03-21T00:24:32.098000Z',\n",
              "     'annotator_id': 1,\n",
              "     'tagged_token_id': '0825a1bf-6a6e-4fa2-be77-8d104701eaed',\n",
              "     'name': 'Ashpat123',\n",
              "     'reason': 'exploration'}],\n",
              "   'model_annotations': []},\n",
              "  {'id': '145f62b1-bbf5-42f1-8ad5-9c7e08337bf0',\n",
              "   'tag_id': 'c06bd022-6ded-44a5-8d90-f17685bb85a1',\n",
              "   'end': 408,\n",
              "   'start': 383,\n",
              "   'example_id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              "   'tag_name': 'Medicine',\n",
              "   'value': 'aluminomagnesium silicate',\n",
              "   'correct': None,\n",
              "   'human_annotations': [{'timestamp': '2020-03-21T00:24:43.692000Z',\n",
              "     'annotator_id': 1,\n",
              "     'tagged_token_id': '145f62b1-bbf5-42f1-8ad5-9c7e08337bf0',\n",
              "     'name': 'Ashpat123',\n",
              "     'reason': 'exploration'}],\n",
              "   'model_annotations': []},\n",
              "  {'id': '243efeb2-723f-4be4-933c-fbbf7e0f9903',\n",
              "   'tag_id': '03eb3e50-d4d8-4261-a60b-fa5aee5deb4a',\n",
              "   'end': 112,\n",
              "   'start': 104,\n",
              "   'example_id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              "   'tag_name': 'MedicalCondition',\n",
              "   'value': 'diarrhea',\n",
              "   'correct': None,\n",
              "   'human_annotations': [{'timestamp': '2020-03-21T00:24:09.423000Z',\n",
              "     'annotator_id': 1,\n",
              "     'tagged_token_id': '243efeb2-723f-4be4-933c-fbbf7e0f9903',\n",
              "     'name': 'Ashpat123',\n",
              "     'reason': 'exploration'}],\n",
              "   'model_annotations': []},\n",
              "  {'id': '281f49d3-879a-4409-b09e-4cfae019af16',\n",
              "   'tag_id': 'c06bd022-6ded-44a5-8d90-f17685bb85a1',\n",
              "   'end': 689,\n",
              "   'start': 679,\n",
              "   'example_id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              "   'tag_name': 'Medicine',\n",
              "   'value': 'kaopectate',\n",
              "   'correct': None,\n",
              "   'human_annotations': [{'timestamp': '2020-03-21T00:25:38.366000Z',\n",
              "     'annotator_id': 1,\n",
              "     'tagged_token_id': '281f49d3-879a-4409-b09e-4cfae019af16',\n",
              "     'name': 'Ashpat123',\n",
              "     'reason': 'exploration'}],\n",
              "   'model_annotations': []},\n",
              "  {'id': '32fdf9e7-63f7-442a-8e25-f08ea4ad94d5',\n",
              "   'tag_id': 'c06bd022-6ded-44a5-8d90-f17685bb85a1',\n",
              "   'end': 23,\n",
              "   'start': 6,\n",
              "   'example_id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              "   'tag_name': 'Medicine',\n",
              "   'value': 'bismuth compounds',\n",
              "   'correct': None,\n",
              "   'human_annotations': [{'timestamp': '2020-03-21T00:23:46.721000Z',\n",
              "     'annotator_id': 1,\n",
              "     'tagged_token_id': '32fdf9e7-63f7-442a-8e25-f08ea4ad94d5',\n",
              "     'name': 'Ashpat123',\n",
              "     'reason': 'exploration'}],\n",
              "   'model_annotations': []},\n",
              "  {'id': '392094d2-febf-4074-a2ca-4c0082f4e5b8',\n",
              "   'tag_id': 'c06bd022-6ded-44a5-8d90-f17685bb85a1',\n",
              "   'end': 37,\n",
              "   'start': 25,\n",
              "   'example_id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              "   'tag_name': 'Medicine',\n",
              "   'value': 'Pepto-Bismol',\n",
              "   'correct': None,\n",
              "   'human_annotations': [{'timestamp': '2020-03-21T00:23:58.861000Z',\n",
              "     'annotator_id': 1,\n",
              "     'tagged_token_id': '392094d2-febf-4074-a2ca-4c0082f4e5b8',\n",
              "     'name': 'Ashpat123',\n",
              "     'reason': 'exploration'}],\n",
              "   'model_annotations': []},\n",
              "  {'id': '450b8c30-cf2e-4774-96d9-58b4160bea38',\n",
              "   'tag_id': '03eb3e50-d4d8-4261-a60b-fa5aee5deb4a',\n",
              "   'end': 470,\n",
              "   'start': 461,\n",
              "   'example_id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              "   'tag_name': 'MedicalCondition',\n",
              "   'value': 'diarrhea ',\n",
              "   'correct': None,\n",
              "   'human_annotations': [],\n",
              "   'model_annotations': []},\n",
              "  {'id': '6b73f683-7130-4e16-bcc2-e3cc8cf89f4d',\n",
              "   'tag_id': 'c06bd022-6ded-44a5-8d90-f17685bb85a1',\n",
              "   'end': 589,\n",
              "   'start': 577,\n",
              "   'example_id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              "   'tag_name': 'Medicine',\n",
              "   'value': 'chemotherapy',\n",
              "   'correct': None,\n",
              "   'human_annotations': [{'timestamp': '2020-03-21T00:25:24.179000Z',\n",
              "     'annotator_id': 1,\n",
              "     'tagged_token_id': '6b73f683-7130-4e16-bcc2-e3cc8cf89f4d',\n",
              "     'name': 'Ashpat123',\n",
              "     'reason': 'exploration'}],\n",
              "   'model_annotations': []},\n",
              "  {'id': '74574b7f-d535-48e1-8651-2708adcfe453',\n",
              "   'tag_id': '03eb3e50-d4d8-4261-a60b-fa5aee5deb4a',\n",
              "   'end': 865,\n",
              "   'start': 853,\n",
              "   'example_id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              "   'tag_name': 'MedicalCondition',\n",
              "   'value': 'constipation',\n",
              "   'correct': None,\n",
              "   'human_annotations': [{'timestamp': '2020-03-21T00:26:31.676000Z',\n",
              "     'annotator_id': 1,\n",
              "     'tagged_token_id': '74574b7f-d535-48e1-8651-2708adcfe453',\n",
              "     'name': 'Ashpat123',\n",
              "     'reason': 'exploration'}],\n",
              "   'model_annotations': []},\n",
              "  {'id': '7572ab8e-ae99-400c-b4ab-ed46fbc9f97e',\n",
              "   'tag_id': 'c06bd022-6ded-44a5-8d90-f17685bb85a1',\n",
              "   'end': 198,\n",
              "   'start': 188,\n",
              "   'example_id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              "   'tag_name': 'Medicine',\n",
              "   'value': 'loperamide',\n",
              "   'correct': None,\n",
              "   'human_annotations': [{'timestamp': '2020-03-21T00:24:17.680000Z',\n",
              "     'annotator_id': 1,\n",
              "     'tagged_token_id': '7572ab8e-ae99-400c-b4ab-ed46fbc9f97e',\n",
              "     'name': 'Ashpat123',\n",
              "     'reason': 'exploration'}],\n",
              "   'model_annotations': []},\n",
              "  {'id': '800e6c6c-0bfb-4819-a25a-34f759753457',\n",
              "   'tag_id': '03eb3e50-d4d8-4261-a60b-fa5aee5deb4a',\n",
              "   'end': 762,\n",
              "   'start': 754,\n",
              "   'example_id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              "   'tag_name': 'MedicalCondition',\n",
              "   'value': 'diarrhea',\n",
              "   'correct': None,\n",
              "   'human_annotations': [{'timestamp': '2020-03-21T00:26:07.368000Z',\n",
              "     'annotator_id': 1,\n",
              "     'tagged_token_id': '800e6c6c-0bfb-4819-a25a-34f759753457',\n",
              "     'name': 'Ashpat123',\n",
              "     'reason': 'exploration'}],\n",
              "   'model_annotations': []},\n",
              "  {'id': '8214556a-7584-4d9b-86cd-5e09137ad904',\n",
              "   'tag_id': '03eb3e50-d4d8-4261-a60b-fa5aee5deb4a',\n",
              "   'end': 880,\n",
              "   'start': 870,\n",
              "   'example_id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              "   'tag_name': 'MedicalCondition',\n",
              "   'value': 'flatulence',\n",
              "   'correct': None,\n",
              "   'human_annotations': [{'timestamp': '2020-03-21T00:26:41.322000Z',\n",
              "     'annotator_id': 1,\n",
              "     'tagged_token_id': '8214556a-7584-4d9b-86cd-5e09137ad904',\n",
              "     'name': 'Ashpat123',\n",
              "     'reason': 'exploration'}],\n",
              "   'model_annotations': []},\n",
              "  {'id': '98968e14-6756-4174-9d3d-7abd58b3aa34',\n",
              "   'tag_id': 'c06bd022-6ded-44a5-8d90-f17685bb85a1',\n",
              "   'end': 833,\n",
              "   'start': 823,\n",
              "   'example_id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              "   'tag_name': 'Medicine',\n",
              "   'value': 'loperamide',\n",
              "   'correct': None,\n",
              "   'human_annotations': [{'timestamp': '2020-03-21T00:26:12.759000Z',\n",
              "     'annotator_id': 1,\n",
              "     'tagged_token_id': '98968e14-6756-4174-9d3d-7abd58b3aa34',\n",
              "     'name': 'Ashpat123',\n",
              "     'reason': 'exploration'}],\n",
              "   'model_annotations': []},\n",
              "  {'id': 'a0a03c7b-cfad-41ee-9f5c-f8a802475994',\n",
              "   'tag_id': '03eb3e50-d4d8-4261-a60b-fa5aee5deb4a',\n",
              "   'end': 853,\n",
              "   'start': 852,\n",
              "   'example_id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              "   'tag_name': 'MedicalCondition',\n",
              "   'value': ' ',\n",
              "   'correct': None,\n",
              "   'human_annotations': [],\n",
              "   'model_annotations': []},\n",
              "  {'id': 'bfbddfd4-38aa-43a7-9366-24c95829ac8c',\n",
              "   'tag_id': '03eb3e50-d4d8-4261-a60b-fa5aee5deb4a',\n",
              "   'end': 469,\n",
              "   'start': 461,\n",
              "   'example_id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              "   'tag_name': 'MedicalCondition',\n",
              "   'value': 'diarrhea',\n",
              "   'correct': None,\n",
              "   'human_annotations': [{'timestamp': '2020-03-21T00:25:06.921000Z',\n",
              "     'annotator_id': 1,\n",
              "     'tagged_token_id': 'bfbddfd4-38aa-43a7-9366-24c95829ac8c',\n",
              "     'name': 'Ashpat123',\n",
              "     'reason': 'exploration'}],\n",
              "   'model_annotations': []},\n",
              "  {'id': 'd7d68c18-0d8e-4547-a2fa-81fdcaf3080e',\n",
              "   'tag_id': '03eb3e50-d4d8-4261-a60b-fa5aee5deb4a',\n",
              "   'end': 543,\n",
              "   'start': 535,\n",
              "   'example_id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              "   'tag_name': 'MedicalCondition',\n",
              "   'value': 'diarrhea',\n",
              "   'correct': None,\n",
              "   'human_annotations': [{'timestamp': '2020-03-21T00:25:12.203000Z',\n",
              "     'annotator_id': 1,\n",
              "     'tagged_token_id': 'd7d68c18-0d8e-4547-a2fa-81fdcaf3080e',\n",
              "     'name': 'Ashpat123',\n",
              "     'reason': 'exploration'}],\n",
              "   'model_annotations': []},\n",
              "  {'id': 'ee956220-42a4-4a91-b9f0-75019c4f5dd9',\n",
              "   'tag_id': 'c06bd022-6ded-44a5-8d90-f17685bb85a1',\n",
              "   'end': 704,\n",
              "   'start': 692,\n",
              "   'example_id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              "   'tag_name': 'Medicine',\n",
              "   'value': 'Racecadotril',\n",
              "   'correct': None,\n",
              "   'human_annotations': [{'timestamp': '2020-03-21T00:25:56.503000Z',\n",
              "     'annotator_id': 1,\n",
              "     'tagged_token_id': 'ee956220-42a4-4a91-b9f0-75019c4f5dd9',\n",
              "     'name': 'Ashpat123',\n",
              "     'reason': 'exploration'}],\n",
              "   'model_annotations': []},\n",
              "  {'id': 'f04a6b7e-8904-405c-9301-e4543238b7a5',\n",
              "   'tag_id': '03eb3e50-d4d8-4261-a60b-fa5aee5deb4a',\n",
              "   'end': 571,\n",
              "   'start': 563,\n",
              "   'example_id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              "   'tag_name': 'MedicalCondition',\n",
              "   'value': 'diarrhea',\n",
              "   'correct': None,\n",
              "   'human_annotations': [{'timestamp': '2020-03-21T00:25:18.043000Z',\n",
              "     'annotator_id': 1,\n",
              "     'tagged_token_id': 'f04a6b7e-8904-405c-9301-e4543238b7a5',\n",
              "     'name': 'Ashpat123',\n",
              "     'reason': 'exploration'}],\n",
              "   'model_annotations': []}],\n",
              " 'classifications': []}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['examples'][0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxng_aQaRDxr",
        "outputId": "87954f66-1dd5-48d8-96a5-1748347c6618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['id', 'content', 'metadata', 'annotations', 'classifications'])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['examples'][0]['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "2CR-fu7kRGir",
        "outputId": "66929f34-8398-43f6-a232-ae6594e0ad0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"While bismuth compounds (Pepto-Bismol) decreased the number of bowel movements in those with travelers' diarrhea, they do not decrease the length of illness.[91] Anti-motility agents like loperamide are also effective at reducing the number of stools but not the duration of disease.[8] These agents should be used only if bloody diarrhea is not present.[92]\\n\\nDiosmectite, a natural aluminomagnesium silicate clay, is effective in alleviating symptoms of acute diarrhea in children,[93] and also has some effects in chronic functional diarrhea, radiation-induced diarrhea, and chemotherapy-induced diarrhea.[45] Another absorbent agent used for the treatment of mild diarrhea is kaopectate.\\n\\nRacecadotril an antisecretory medication may be used to treat diarrhea in children and adults.[86] It has better tolerability than loperamide, as it causes less constipation and flatulence.[94]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['examples'][0]['annotations'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqlzt_eBRI2a",
        "outputId": "fbd9c6ed-8b59-450d-9591-32f30e3bfb86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0825a1bf-6a6e-4fa2-be77-8d104701eaed',\n",
              " 'tag_id': 'c06bd022-6ded-44a5-8d90-f17685bb85a1',\n",
              " 'end': 371,\n",
              " 'start': 360,\n",
              " 'example_id': '18c2f619-f102-452f-ab81-d26f7e283ffe',\n",
              " 'tag_name': 'Medicine',\n",
              " 'value': 'Diosmectite',\n",
              " 'correct': None,\n",
              " 'human_annotations': [{'timestamp': '2020-03-21T00:24:32.098000Z',\n",
              "   'annotator_id': 1,\n",
              "   'tagged_token_id': '0825a1bf-6a6e-4fa2-be77-8d104701eaed',\n",
              "   'name': 'Ashpat123',\n",
              "   'reason': 'exploration'}],\n",
              " 'model_annotations': []}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = []\n",
        "for example in data['examples']:\n",
        "  temp_dict = {}\n",
        "  temp_dict['text'] = example['content']\n",
        "  temp_dict['entities'] = []\n",
        "  for annotation in example['annotations']:\n",
        "    start = annotation['start']\n",
        "    end = annotation['end']\n",
        "    label = annotation['tag_name'].upper()\n",
        "    temp_dict['entities'].append((start, end, label))\n",
        "  training_data.append(temp_dict)\n",
        "\n",
        "print(training_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8S2ynwURKjS",
        "outputId": "e7acc51a-2822-4cb5-8a20-fd9e720cb7d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \"While bismuth compounds (Pepto-Bismol) decreased the number of bowel movements in those with travelers' diarrhea, they do not decrease the length of illness.[91] Anti-motility agents like loperamide are also effective at reducing the number of stools but not the duration of disease.[8] These agents should be used only if bloody diarrhea is not present.[92]\\n\\nDiosmectite, a natural aluminomagnesium silicate clay, is effective in alleviating symptoms of acute diarrhea in children,[93] and also has some effects in chronic functional diarrhea, radiation-induced diarrhea, and chemotherapy-induced diarrhea.[45] Another absorbent agent used for the treatment of mild diarrhea is kaopectate.\\n\\nRacecadotril an antisecretory medication may be used to treat diarrhea in children and adults.[86] It has better tolerability than loperamide, as it causes less constipation and flatulence.[94]\", 'entities': [(360, 371, 'MEDICINE'), (383, 408, 'MEDICINE'), (104, 112, 'MEDICALCONDITION'), (679, 689, 'MEDICINE'), (6, 23, 'MEDICINE'), (25, 37, 'MEDICINE'), (461, 470, 'MEDICALCONDITION'), (577, 589, 'MEDICINE'), (853, 865, 'MEDICALCONDITION'), (188, 198, 'MEDICINE'), (754, 762, 'MEDICALCONDITION'), (870, 880, 'MEDICALCONDITION'), (823, 833, 'MEDICINE'), (852, 853, 'MEDICALCONDITION'), (461, 469, 'MEDICALCONDITION'), (535, 543, 'MEDICALCONDITION'), (692, 704, 'MEDICINE'), (563, 571, 'MEDICALCONDITION')]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data[0]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "A9ZOuRYQROtS",
        "outputId": "f669634c-75e7-4411-fb41-c4b474dba74e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"While bismuth compounds (Pepto-Bismol) decreased the number of bowel movements in those with travelers' diarrhea, they do not decrease the length of illness.[91] Anti-motility agents like loperamide are also effective at reducing the number of stools but not the duration of disease.[8] These agents should be used only if bloody diarrhea is not present.[92]\\n\\nDiosmectite, a natural aluminomagnesium silicate clay, is effective in alleviating symptoms of acute diarrhea in children,[93] and also has some effects in chronic functional diarrhea, radiation-induced diarrhea, and chemotherapy-induced diarrhea.[45] Another absorbent agent used for the treatment of mild diarrhea is kaopectate.\\n\\nRacecadotril an antisecretory medication may be used to treat diarrhea in children and adults.[86] It has better tolerability than loperamide, as it causes less constipation and flatulence.[94]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data[0]['entities']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeMkpgV6RQML",
        "outputId": "79942532-c3b8-4994-918e-b58079bf1eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(360, 371, 'MEDICINE'),\n",
              " (383, 408, 'MEDICINE'),\n",
              " (104, 112, 'MEDICALCONDITION'),\n",
              " (679, 689, 'MEDICINE'),\n",
              " (6, 23, 'MEDICINE'),\n",
              " (25, 37, 'MEDICINE'),\n",
              " (461, 470, 'MEDICALCONDITION'),\n",
              " (577, 589, 'MEDICINE'),\n",
              " (853, 865, 'MEDICALCONDITION'),\n",
              " (188, 198, 'MEDICINE'),\n",
              " (754, 762, 'MEDICALCONDITION'),\n",
              " (870, 880, 'MEDICALCONDITION'),\n",
              " (823, 833, 'MEDICINE'),\n",
              " (852, 853, 'MEDICALCONDITION'),\n",
              " (461, 469, 'MEDICALCONDITION'),\n",
              " (535, 543, 'MEDICALCONDITION'),\n",
              " (692, 704, 'MEDICINE'),\n",
              " (563, 571, 'MEDICALCONDITION')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data[0]['text'][360:371]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rCDEoSieRR27",
        "outputId": "348a5f9a-c724-47df-ba3c-1e7175e96844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Diosmectite'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "\n",
        "nlp = spacy.blank(\"en\") # load a new spacy model\n",
        "doc_bin = DocBin()"
      ],
      "metadata": {
        "id": "BGY9A_ehRTuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.util import filter_spans\n",
        "\n",
        "for training_example  in tqdm(training_data):\n",
        "    text = training_example['text']\n",
        "    labels = training_example['entities']\n",
        "    doc = nlp.make_doc(text)\n",
        "    ents = []\n",
        "    for start, end, label in labels:\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "        if span is None:\n",
        "            print(\"Skipping entity\")\n",
        "        else:\n",
        "            ents.append(span)\n",
        "    filtered_ents = filter_spans(ents)\n",
        "    doc.ents = filtered_ents\n",
        "    doc_bin.add(doc)\n",
        "\n",
        "doc_bin.to_disk(\"train.spacy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmKY3LiaRVmJ",
        "outputId": "dc87d46a-63e6-4b0f-eb1c-dff629a3278b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|██▌       | 8/31 [00:00<00:00, 72.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping entity\n",
            "Skipping entity\n",
            "Skipping entity\n",
            "Skipping entity\n",
            "Skipping entity\n",
            "Skipping entity\n",
            "Skipping entity\n",
            "Skipping entity\n",
            "Skipping entity\n",
            "Skipping entity\n",
            "Skipping entity\n",
            "Skipping entity\n",
            "Skipping entity\n",
            "Skipping entity\n",
            "Skipping entity\n",
            "Skipping entity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31/31 [00:00<00:00, 131.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config base_config.cfg config.cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3VigVcERaEs",
        "outputId": "9a5dcb52-1b91-4932-b4c3-8776ce58620e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-28 17:51:50.406240: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[33mUsage: \u001b[0mpython \u001b[1;32m-m\u001b[0m spacy init fill-config \n",
            "           [OPTIONS] BASE_PATH [OUTPUT_FILE]\n",
            "\u001b[2mTry \u001b[0m\u001b[2;34m'python \u001b[0m\u001b[1;2;34m-m\u001b[0m\u001b[2;34m spacy init fill-config \u001b[0m\u001b[1;2;34m-\u001b[0m\u001b[1;2;34m-help\u001b[0m\u001b[2;34m'\u001b[0m\u001b[2m for help.\u001b[0m\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m Error \u001b[0m\u001b[31m─────────────────────────────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m Invalid value for 'BASE_PATH': File 'base_config.cfg' does not exist.        \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train config.cfg --output ./ --paths.train ./train.spacy --paths.dev ./train.spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwBAIlTARdNB",
        "outputId": "a2a85b5b-972b-4a79-cc65-4c10ddc2db81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-28 17:52:02.886320: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[33mUsage: \u001b[0mpython \u001b[1;32m-m\u001b[0m spacy train [OPTIONS] CONFIG_PATH\n",
            "\u001b[2mTry \u001b[0m\u001b[2;34m'python \u001b[0m\u001b[1;2;34m-m\u001b[0m\u001b[2;34m spacy train \u001b[0m\u001b[1;2;34m-\u001b[0m\u001b[1;2;34m-help\u001b[0m\u001b[2;34m'\u001b[0m\u001b[2m for help.\u001b[0m\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m Error \u001b[0m\u001b[31m─────────────────────────────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m Invalid value for 'CONFIG_PATH': Path 'config.cfg' does not exist.           \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_ner = spacy.load(\"model-best\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "PEqjBuceReyq",
        "outputId": "b14e1de6-459d-4b7d-9c9c-ca5e239e9a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-9c948d047251>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp_ner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model-best\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'model-best'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp_ner(\"While bismuth compounds (Pepto-Bismol) decreased the number of bowel movements in those with travelers' diarrhea, they do not decrease the length of illness.[91] Anti-motility agents like loperamide are also effective at reducing the number of stools but not the duration of disease.[8] These agents should be used only if bloody diarrhea is not present.\")\n",
        "\n",
        "colors = {\"PATHOGEN\": \"#F67DE3\", \"MEDICINE\": \"#7DF6D9\", \"MEDICALCONDITION\":\"#a6e22d\"}\n",
        "options = {\"colors\": colors}\n",
        "\n",
        "spacy.displacy.render(doc, style=\"ent\", options= options, jupyter=True)"
      ],
      "metadata": {
        "id": "Q0Rz0S6qRqjU",
        "outputId": "470957e5-b297-4daf-dffb-5eac94fe6520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-9db15d153870>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"While bismuth compounds (Pepto-Bismol) decreased the number of bowel movements in those with travelers' diarrhea, they do not decrease the length of illness.[91] Anti-motility agents like loperamide are also effective at reducing the number of stools but not the duration of disease.[8] These agents should be used only if bloody diarrhea is not present.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"PATHOGEN\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"#F67DE3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MEDICINE\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"#7DF6D9\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MEDICALCONDITION\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"#a6e22d\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"colors\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nlp_ner' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iZOOFVaTRrP1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "IfP0dNsNGAfO",
        "3RUH1TVZ79tI",
        "TsMEBS1D-7e3",
        "9BsSCBkMPsX6",
        "O66VyInjZgDZ",
        "eP4uVBuQMRtH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0633eef4dc234f5db838a79bcf53f9c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15cb804d51834b9db7a9134268a80e5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "17784b2f99d94faa86246a040b72296a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1875c336e93b4923a013a345606343d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38509d93cd044da5970ea0e3e8dedd5d",
              "IPY_MODEL_279b590d3b794b2eb089e78e51b94a22"
            ],
            "layout": "IPY_MODEL_e35ff9b168be45f285d5e6d6b0858024"
          }
        },
        "279b590d3b794b2eb089e78e51b94a22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d43e5c48b1ad4ef78c9e574d1e3f8670",
            "placeholder": "​",
            "style": "IPY_MODEL_4a27f9cef77f419499af21dc346c4a5e",
            "value": " 996k/996k [00:00&lt;00:00, 1.73MB/s]"
          }
        },
        "29209ef7e663423ea88b0f5c4c283bb3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fec018a8c914a099090a1fd33a857e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88502a3f6f05453a8df6930c82d56ed0",
              "IPY_MODEL_b0fb4cbb7e764cb89868889adef8906b"
            ],
            "layout": "IPY_MODEL_ede1437be3714135916989b756037f74"
          }
        },
        "31d079fc9b3b46b4a05fb92ed3028030": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ff1a9f3e08f428e8b693ac80a0025dd",
              "IPY_MODEL_5106c02be5814b3796dd3f9f22cba957"
            ],
            "layout": "IPY_MODEL_29209ef7e663423ea88b0f5c4c283bb3"
          }
        },
        "34ccc1bfe764485f848be7162f846217": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38509d93cd044da5970ea0e3e8dedd5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84b063e752734300bd7a683e82fc1567",
            "max": 995526,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15cb804d51834b9db7a9134268a80e5c",
            "value": 995526
          }
        },
        "49e82674d6aa43e881ebff1c425ce041": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "4a27f9cef77f419499af21dc346c4a5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5106c02be5814b3796dd3f9f22cba957": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0633eef4dc234f5db838a79bcf53f9c0",
            "placeholder": "​",
            "style": "IPY_MODEL_86ee11de49f54bf0bd3654a9fa460d60",
            "value": " 714M/714M [00:12&lt;00:00, 56.8MB/s]"
          }
        },
        "5ff1a9f3e08f428e8b693ac80a0025dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17784b2f99d94faa86246a040b72296a",
            "max": 714314041,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49e82674d6aa43e881ebff1c425ce041",
            "value": 714314041
          }
        },
        "84b063e752734300bd7a683e82fc1567": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86ee11de49f54bf0bd3654a9fa460d60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88502a3f6f05453a8df6930c82d56ed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b0bdb8d58284c13b97b29e1f961fef8",
            "max": 625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad43b0269b3e448a89fdae9d37042aed",
            "value": 625
          }
        },
        "9b0bdb8d58284c13b97b29e1f961fef8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad43b0269b3e448a89fdae9d37042aed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "b0fb4cbb7e764cb89868889adef8906b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34ccc1bfe764485f848be7162f846217",
            "placeholder": "​",
            "style": "IPY_MODEL_e49538c37df640738b5281b6aaa8a6b3",
            "value": " 625/625 [00:01&lt;00:00, 572B/s]"
          }
        },
        "d43e5c48b1ad4ef78c9e574d1e3f8670": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e35ff9b168be45f285d5e6d6b0858024": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e49538c37df640738b5281b6aaa8a6b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ede1437be3714135916989b756037f74": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}